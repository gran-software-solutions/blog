"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[9450],{6029:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2024/01/05/3-important-reasons-why-i-made-my-own-tracking-tool","metadata":{"permalink":"/2024/01/05/3-important-reasons-why-i-made-my-own-tracking-tool","editUrl":"https://github.com/gran-software-solutions/blog/tree/main/blog/2024-01-05-3-important-reasons-why-i-made-my-own-tracking-tool/index.mdx","source":"@site/blog/2024-01-05-3-important-reasons-why-i-made-my-own-tracking-tool/index.mdx","title":"3 Important Reasons Why I Made My Own Tracking Tool","description":"A timeline with blocks of time on it","date":"2024-01-05T00:00:00.000Z","formattedDate":"January 5, 2024","tags":[{"label":"productivity","permalink":"/tags/productivity"}],"readingTime":2.465,"hasTruncateMarker":true,"authors":[{"name":"Greg Glazewski","title":"Senior engineer @ GRAN Software Solutions GmbH","url":"https://github.com/gglazewskigran","imageURL":"https://github.com/gglazewskigran.png","key":"gglazewskigran"}],"frontMatter":{"title":"3 Important Reasons Why I Made My Own Tracking Tool","authors":"gglazewskigran","tags":["productivity"],"enableComments":true},"nextItem":{"title":"HATEOAS Hands-on for Backend and Frontend","permalink":"/2023/05/24/hateoas-hands-on-backend-frontend"}},"content":"![A timeline with blocks of time on it](./ss1.png \\"Tracking time is fun\\")\\n\\nI\u2019m a software developer and a freelancer. Tracking time is something I\u2019m doing every day.\\n\\n\x3c!--truncate--\x3e\\n\\nExporting timesheets and adjusting them when clients argue. Cross-checking tracked time with tickets and contracts (did I overshoot my hours?).\\nConverting time to decimal values for my invoices (btw. It\u2019s hours + minutes / 60).\\n\\n```\\n10:33 = 10 + (33 / 60) = 10.55\\n```\\nIt may not be much but enough for me to be annoyed with the tools I used to use.\\n\\n## The Urge to Create\\nAt one point I said to myself: I\u2019m a software developer, I develop software. And so I asked my friend to team up and build our own tool to track time. We wanted to implement all the features we expected, all the quality of life improvements we wanted and craft it so to call it truly ours.\\n\\nFast forward **6 weeks** and the tool is online (you can check it out here: https://sheetty.com). From the idea to actually using it at our work.\\n\\nBuilding your own stuff is really satisfying. The feeling of freedom, the power to create, making something out of nothing. The satisfaction pouring from the process of creation is almost addicting.\\n\\nHonestly, this is the first reason we even started. We **wanted** to create something. And if you haven\u2019t yet, I really recommend you to start with anything and just create. The feeling of accomplishment will sooth all the stress, despair and sleepless nights.\\n\\n<video autoPlay loop>\\n    <source src=\\"/vid1.mp4\\"/>\\n</video>\\n\\nWe are happy with what we have so far and time tracking is now fun\\n\\n### YAGNI\\nAs a software dev, I know very well the \u201cYou aren\u2019t gonna need it\u201d principle ([Wikipedia](https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it)). The issue I had with my old time trackers is that they included everything. The ever growing feature list was overwhelming and those that I really needed were hidden in a swamp of buttons, menus, tabs and options. More often than not I didn\u2019t even know that they support what I wanted just because there was no way to figure this out.\\n\\n![Search for functions and pages](./ss2.png \\"Access what you want, fast\\")\\n\\nModern UI and UX teach us that you access the information you are looking for almost instantly. Just double press shift in Intellij, command + space on mac or ctrl + k on any modern website and you\'ll find anything you need. Two things I expect from any software is to provide:\\n- Only what is needed\\n- Fast\\n\\nAnd that was a strong motivation for us. To make time tracking simple and fast.\\n\\n## TL;DR\\n\\nThe inner pressure to consume less and create more, an offer of much more than you actually need, a workflow that\u2019s not aligned with the way you work. These are strong signals that you maybe should not ignore but use them to bring value to this world.\\n\\nThanks for reading.\\n\\nCheers!"},{"id":"/2023/05/24/hateoas-hands-on-backend-frontend","metadata":{"permalink":"/2023/05/24/hateoas-hands-on-backend-frontend","editUrl":"https://github.com/gran-software-solutions/blog/tree/main/blog/2023-05-24-hateoas-hands-on-backend-frontend/index.mdx","source":"@site/blog/2023-05-24-hateoas-hands-on-backend-frontend/index.mdx","title":"HATEOAS Hands-on for Backend and Frontend","description":"HATEOAS","date":"2023-05-24T00:00:00.000Z","formattedDate":"May 24, 2023","tags":[{"label":"REST API","permalink":"/tags/rest-api"},{"label":"technology","permalink":"/tags/technology"}],"readingTime":7.625,"hasTruncateMarker":true,"authors":[{"name":"Greg Glazewski","title":"Senior engineer @ GRAN Software Solutions GmbH","url":"https://github.com/gglazewskigran","imageURL":"https://github.com/gglazewskigran.png","key":"gglazewskigran"}],"frontMatter":{"title":"HATEOAS Hands-on for Backend and Frontend","authors":"gglazewskigran","tags":["REST API","technology"],"enableComments":true},"prevItem":{"title":"3 Important Reasons Why I Made My Own Tracking Tool","permalink":"/2024/01/05/3-important-reasons-why-i-made-my-own-tracking-tool"},"nextItem":{"title":"To Gzip or Not to Gzip? A Guide to Using Compression in Vert.x REST APIs","permalink":"/2023/05/16/gzip-compression-guide-vertx-rest-api"}},"content":"![HATEOAS](./banner.jpeg)\\n\\nRESTful API without HATEOAS is not a RESTful API by definition. The majority of developers refrain from using HATEOAS because it comes with extra complexity, its value is questionable and it\u2019s not popular in general.\\n\\nLet\u2019s explore HATEOAS hands-on and see if those concerns hold. You will learn how to implement HATEOAS on the server side and the client side using libraries and get access to working code on GitHub.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nWhile the RESTfulness of an API falls on a spectrum, incorporating HATEOAS brings it closer to the ideal. By adopting HATEOAS principles, you can achieve better coordination, simplify development, and enhance the overall collaborative experience.\\nIf you want to have:\\n- a robust API by making your clients harder to break\\n- self-documenting endpoints that improve communication\\n- a standard convention to leverage rich tooling and libraries\\n\\nRead further and see what I propose to solve the HATEOAS complexity problem.\\n\\n\\n## HATEOAS in Backend\\nIn our example we will use Kotlin and Spring, because I think this is a pretty popular and easy to use setup.\\n\\nI would suggest you start with designing the API first. For our example we will use OpenAPI 3.0 specification.\\n\\n![Borrow API](./borrow.png)\\n\\nIn our example, we will use Kotlin and Spring, because I think this is a pretty popular and easy-to-use setup.\\nI would suggest you start with designing the API first. For our example, we will use OpenAPI 3.0 specification.\\nWe\u2019re going to create a Borrow API to manage things that you lend so that you will never lose a thing! We\u2019re going to have a Borrowee (because it sounds nice) and an Item. We could have many endpoints to cover many use cases like so:\\n\\n```\\nGET /items\\nGET /items/{name}\\nGET /borrowees\\nGET /borrowees/{name}\\nPOST /item\\nPOST /borrowee\\nPUT /item/{name}\\nDELETE /item/{name}\\nDELETE /borrowee/{name}\\nGET /items/by/{name}\\n```\\n\\nBut for sake of simplicity we will use only a few, just enough to explain the issue.\\nYou can find the full OAS 3.0 specs in the repository (look in the comment section down below) in `resources/borrow.yaml`.\\nOpenAPI spec is the place where you already need to think about HATEOAS. A good idea is to sketch first what links you want to attach where. And don\u2019t forget to leverage JSON Schema i.e. define links in components, not in paths.\\n\\n## OpenAPI\\n\\nThe Borrow API has only two objects (Item, Borrowee) and we could define them like so:\\n\\n```yaml\\ncomponents:\\n  schemas:\\n    Item:\\n      type: object\\n      properties:\\n        name:\\n          type: string\\n        borrowee:\\n          type: string\\n        links:\\n          type: object\\n          properties:\\n            self:\\n              $ref: \'#/components/links/item-from-name\'\\n            borrowee:\\n              $ref: \'#/components/links/borrowee-from-item\'\\n    Borrowee:\\n      type: object\\n      properties:\\n        name:\\n          type: string\\n        links:\\n          type: object\\n          properties:\\n            self:\\n              $ref: \'#/components/links/borrowee-from-name\'\\n            items:\\n              $ref: \'#/components/links/item-from-borrowee\'\\n```\\n\\nYou can see that there is the property `links` which defines two other properties. The `self` and the `borrowee` for Item and `self` and `items` for Borrowee. All of those refer a component from the JSON Schema (`$ref: \u2026`):\\n\\n```yaml\\n  links:\\n    borrowee-from-item:\\n      operationId: get-a-borrowee\\n      parameters:\\n        name: $response.body#/borrowee\\n    borrowee-from-name:\\n      operationId: get-all-borrowees\\n      parameters:\\n        name: $response.body#/name\\n    item-from-name:\\n      operationId: get-an-item\\n      parameters:\\n        name: $response.body#/name\\n    item-from-borrowee:\\n      operationId: get-items-by-borrowee\\n      parameters:\\n        name: $response.body#/name\\n```\\n\\nLet\u2019s decode this part. A link is made of 3 parts:\\n- Name\\n- Operation id\\n- Parameters\\n\\nTake the first one as an example. `borrowee-from-item` is the name of the link. It refers the operation with the id `get-a-borrowee`. This operation id is a concrete path from the specification. In our case, it\u2019s this:\\n\\n```yaml\\n  \'/borrowee/{name}\':\\n    get:\\n      operationId: get-a-borrowee\\n      responses:\\n        \'200\':\\n          description: OK\\n          content:\\n            application/json:\\n              schema:\\n                $ref: \'#/components/schemas/Borrowee\'\\n              examples:\\n                Example 1:\\n                  value:\\n                    name: John\\n                    links:\\n                      self: /borrowee/John\\n                      items: /items/by/John\\n```\\n\\nLast but not least, the list of parameters that are used to build the URI e.g. when you have path parameters in your endpoint. Our URI is `/borrowee/{name}`. So the link has a `name` parameter defined. Its value comes from the body of the response. If the response looks like so:\\n\\n```json\\n{\\n\\t\\"name\\": \\"blue pen\\",\\n\\t\\"borrowee\\": \\"john\\"\\n}\\n```\\n\\nYou can address this property using this syntax: `$response.body#/borrowee`.\\nThis may sound confusing by now. But it\u2019s pretty simple if you approach this in 4 steps:\\n- Define your components in the schema\\n- Define your paths and which components they return\\n- Define `self` links for every component\\n- Define which components should be linked to other components\\n\\nNow that you have your OpenAPI specs, you have two options. Generate code from it (e.g. models or even interfaces for your controllers) or write your code based on the specifications. This is an article about HATEOAS and not OpenAPI code generation, so let\u2019s just write some code.\\n\\n# Kotlin\\n\\nWe will need two models:\\n\\n```kotlin\\ndata class Item(val name: String, val borrowee: String) : RepresentationModel<Item>()\\ndata class Borrowee(val name: String) : RepresentationModel<Borrowee>()\\n```\\n\\nThose will extend the `org.springframework.hateoas.RepresentationModel`. To use it, you can include this starter in your `build.gradle`:\\n```kotlin\\nimplementation(\\"org.springframework.boot:spring-boot-starter-hateoas\\")\\n```\\n\\nNow create two controllers. `ItemController` and `BorroweeController`. Let\u2019s first add an endpoint returning all items.\\n\\n```kotlin\\n    @GetMapping(\\"/items\\")\\n    fun getItems(\\n        @RequestParam(value = \\"page\\", defaultValue = \\"0\\") page: Int = 0,\\n        @RequestParam(value = \\"size\\", defaultValue = \\"2\\") size: Int = 2,\\n    ): ResponseEntity<CustomCollectionModel<Item>> {\\n        val items = listOf(\\n            Item(\\"Blue Pen\\", \\"John\\").apply {\\n                add(linkTo(methodOn(ItemController::class.java).getItem(this.name)).withSelfRel())\\n                add(linkTo(methodOn(BorroweeController::class.java).getBorrowee(this.borrowee)).withRel(\\"borrowee\\"))\\n            },\\n            Item(\\"Notebook\\", \\"John\\").apply {\\n                add(linkTo(methodOn(ItemController::class.java).getItem(this.name)).withSelfRel())\\n                add(linkTo(methodOn(BorroweeController::class.java).getBorrowee(this.borrowee)).withRel(\\"borrowee\\"))\\n            },\\n            Item(\\"Headphones\\", \\"John\\").apply {\\n                add(linkTo(methodOn(ItemController::class.java).getItem(this.name)).withSelfRel())\\n                add(linkTo(methodOn(BorroweeController::class.java).getBorrowee(this.borrowee)).withRel(\\"borrowee\\"))\\n            }\\n        )\\n\\n        val pageable = PageRequest.of(page, size)\\n        val startIndex = pageable.offset.toInt()\\n        val endIndex = min(startIndex + pageable.pageSize, items.size)\\n        val selected = items.subList(startIndex, endIndex)\\n\\n        val totalPages = ceil(items.size.toDouble() / size).toInt()\\n\\n        return ResponseEntity.ok(\\n            CustomCollectionModel(selected).apply {\\n                add(linkTo(methodOn(ItemController::class.java).getItems(page, size)).withSelfRel())\\n\\n                if (page > 0) {\\n                    add(linkTo(methodOn(ItemController::class.java).getItems(0, size)).withRel(\\"first\\"))\\n                    add(linkTo(methodOn(ItemController::class.java).getItems(page - 1, size)).withRel(\\"prev\\"))\\n                }\\n\\n                if (page < totalPages - 1) {\\n                    add(linkTo(methodOn(ItemController::class.java).getItems(page + 1, size)).withRel(\\"next\\"))\\n                    add(linkTo(methodOn(ItemController::class.java).getItems(totalPages - 1, size)).withRel(\\"last\\"))\\n                }\\n            }\\n        )\\n    }\\n```\\n\\nWe use RequestParams for page and size. This is used for pagination. In the bottom of the function you can see how we leverage HATEOAS to provide clients with automated pagination:\\n\\n```kotlin\\n        return ResponseEntity.ok(\\n            CustomCollectionModel(selected).apply {\\n                add(linkTo(methodOn(ItemController::class.java).getItems(page, size)).withSelfRel())\\n\\n                if (page > 0) {\\n                    add(linkTo(methodOn(ItemController::class.java).getItems(0, size)).withRel(\\"first\\"))\\n                    add(linkTo(methodOn(ItemController::class.java).getItems(page - 1, size)).withRel(\\"prev\\"))\\n                }\\n\\n                if (page < totalPages - 1) {\\n                    add(linkTo(methodOn(ItemController::class.java).getItems(page + 1, size)).withRel(\\"next\\"))\\n                    add(linkTo(methodOn(ItemController::class.java).getItems(totalPages - 1, size)).withRel(\\"last\\"))\\n                }\\n            }\\n        )\\n```\\n\\nThe `CustomCollectionModel is a wrapper around the response. This has two important tasks:\\n- Allow for adding extra fields to the model (this is used to add links)\\n- Allow us to define the response as we wish. In our case we want to have something like:\\n\\n```json\\n{\\n\\t\\"data\\": []\\n\\t\\"_links\\": {}\\n}\\n```\\n\\nWithout this wrapper, we would not be able to add `_links`, and without our custom wrapper instead of `data` we would see the `_embedded` property.\\nThe first few lines of code that define the `items` variable is just a database replacement. But it does show HATEOAS in action:\\n\\n```kotlin\\nItem(\\"Blue Pen\\", \\"John\\").apply {\\n    add(linkTo(methodOn(ItemController::class.java).getItem(this.name)).withSelfRel())\\n    add(linkTo(methodOn(BorroweeController::class.java).getBorrowee(this.borrowee)).withRel(\\"borrowee\\"))\\n}\\n```\\nBecause our model extends the `RepresentationModel`, we can add extra properties i.e. links. Those are generated from the controller and allow us to define the `rel` name. The code from above would generate something similar to:\\n```json\\n{\\n  \\"name\\": \\"Blue Pen\\",\\n  \\"borrowee\\": \\"John\\",\\n  \\"_links\\": {\\n    \\"self\\": \\"/item/Blue Pen\\",\\n    \\"borrowee\\": \\"/borrowee/John\\"\\n  }\\n}\\n```\\nWe will do the same for `BorroweeController`. But to allow clients to discover the API entirely, we need a main hub with all the endpoints that the client can access. Let\u2019s create a `HATEOASController`:\\n\\n```kotlin\\n@RestController\\n@CrossOrigin(origins =  [\\"http://localhost:3000\\"] )\\nclass HATEOASController {\\n    @GetMapping(\\"/\\")\\n    fun getItem(): RepresentationModel<*> {\\n        return RepresentationModel.of(String).apply {\\n            add(linkTo(methodOn(ItemController::class.java).getItems()).withRel(\\"items\\"))\\n            add(linkTo(methodOn(BorroweeController::class.java).getBorrowees()).withRel(\\"borrowees\\"))\\n        }\\n    }\\n}\\n```\\n\\nGreat! If you check out the code, you can simply run the app and call those endpoints from the browser or your favorite rest clients like Postman or Insomnia.\\n\\n# HATEOAS on Client Side\\n\\nFor the client we will write a simple React app that uses Axiom to read from our API.\\nSimply create a new React app and add your component.\\n\\n```typescript\\n\\nexport default function Index() {\\n    const [data, setData] = useState<Data>({ items: [], borrowees: [] });\\n\\n    useEffect(() => {\\n        fetchData();\\n    }, []);\\n\\n    const fetchData = async () => {\\n        try {\\n            const loadedData = await loader();\\n            setData(loadedData);\\n            console.log(loadedData);\\n        } catch (error) {\\n            console.error(\\"Error fetching data:\\", error);\\n        }\\n    };\\n\\n    return (\\n        <div>\\n            <h2>Items</h2>\\n            <ul>\\n                {data.items.map((item, index) => (\\n                    <li key={index}>{item.name}</li>\\n                ))}\\n            </ul>\\n\\n            <h2>Borrowees</h2>\\n            <ul>\\n                {data.borrowees.map((borrowee, index) => (\\n                    <li key={index}>{borrowee.name}</li>\\n                ))}\\n            </ul>\\n        </div>\\n    );\\n}\\n```\\n\\nThe interesting part is inside the `loader()` function:\\n```typescript\\nexport async function loader(): Promise<Data> {\\n    const response: AxiosResponse = await axios.get(\'http://localhost:8080\');\\n\\n    const itemsResponse: AxiosResponse<{ data: Item[] }> = await axios.get(response.data._links.items.href);\\n    const borroweesResponse: AxiosResponse<{ data: Item[] }> = await axios.get(response.data._links.borrowees.href);\\n\\n    const items: Item[] = itemsResponse.data.data;\\n    const borrowees: Item[] = borroweesResponse.data.data;\\n\\n    return {\\n        items,\\n        borrowees\\n    };\\n}\\n```\\n\\nAs you can see, it only requires the base URL i.e. `localhost:8080`. Every other endpoint is discovered automatically. You only need to call it once to see which resources you want to get.\\nThe result is this:\\n\\n![Result](./result.png)\\n\\nWe got all items and borrowees, without even knowing what endpoints we use!\\n\\n# Conclusion\\n\\nAs you can see, HATEOAS is not a scary monster from under the bed. With proper tooling it\u2019s a breeze to configure and use.\\nWhat are your thoughts on HATEOAS? Please let me know and if you think someone could benefit from it, I will be grateful if you share this article."},{"id":"/2023/05/16/gzip-compression-guide-vertx-rest-api","metadata":{"permalink":"/2023/05/16/gzip-compression-guide-vertx-rest-api","editUrl":"https://github.com/gran-software-solutions/blog/tree/main/blog/2023-05-16-gzip-compression-guide-vertx-rest-api/index.mdx","source":"@site/blog/2023-05-16-gzip-compression-guide-vertx-rest-api/index.mdx","title":"To Gzip or Not to Gzip? A Guide to Using Compression in Vert.x REST APIs","description":"Elephants","date":"2023-05-16T00:00:00.000Z","formattedDate":"May 16, 2023","tags":[{"label":"REST API","permalink":"/tags/rest-api"},{"label":"technology","permalink":"/tags/technology"},{"label":"vertx","permalink":"/tags/vertx"}],"readingTime":4.465,"hasTruncateMarker":true,"authors":[{"name":"Dusan Odalovic","title":"Senior engineer @ GRAN Software Solutions GmbH","url":"https://github.com/dodalovicgran","imageURL":"https://github.com/dodalovicgran.png","key":"dodalovicgran"}],"frontMatter":{"title":"To Gzip or Not to Gzip? A Guide to Using Compression in Vert.x REST APIs","authors":"dodalovicgran","tags":["REST API","technology","vertx"],"enableComments":true},"prevItem":{"title":"HATEOAS Hands-on for Backend and Frontend","permalink":"/2023/05/24/hateoas-hands-on-backend-frontend"},"nextItem":{"title":"Why you should design your REST APIs with concurrency in mind","permalink":"/2023/03/27/concurrency-in-rest-apis"}},"content":"![Elephants](./elephants.png)\\n\\nGzip compression is a way of making files smaller on the web, which can improve the performance and efficiency of your vert.x based REST APIs. However, it also has some drawbacks and trade-offs that\\nyou need to consider.\\n\\nIn this blog post, I will explain what gzip compression is, how it works, and how you can enable or disable it in your vert.x REST API.\\nI will also discuss the pros and cons of enabling or disabling gzip compression in different scenarios.\\n\\n\x3c!--truncate--\x3e\\n\\n## What is gzip?\\n\\nGzip compression is a way of making files smaller so they can travel faster on the web. It is like putting clothes in a vacuum bag and sucking out the air to make them take up less space. Gzip compression can make\\nfiles up to 10 times smaller, depending on what\u2019s in them.\\n\\nGzip compression works by using a special code that finds and removes parts of the files that are repeated or not needed. It also adds some information at the beginning and end of the files to tell how they\\nwere compressed and how to unpack them. The files are then divided into smaller pieces that are easier to send and receive.\\n\\nGzip compression is used by many websites and browsers, and can make web pages load faster and use less data. When a browser asks for a web page, it can tell the website if it can handle gzip compression.\\nIf so, the website can pack the web page using gzip and send it to the browser. The browser can then unpack the web page using gzip and show it as usual. This way, both the website and the browser can save\\ntime and money.\\n\\n## Benefits of enabling gzip compression\\n\\nHere are the main benefits of enabling gzip compression:\\n\\n* **Faster page load times**: Gzip compression can make web pages load faster because they are smaller and take less time to travel over the network.\\n* **Reduced bandwidth usage**: Gzip compression can reduce the amount of data that is transferred over the network between the website and the browser.\\n* **Improved user experience**: Gzip compression can improve the user experience by making web pages load faster and use less data.\\n* **Improved SEO ranking**: Gzip compression can improve the SEO ranking of a website because it makes web pages load faster and use less data (doesn\'t matter for REST APIs).\\n* **Compatibility with most browsers and clients**: Gzip compression is supported by most browsers and clients, so it can be used on most websites.\\n\\n## Drawbacks of enabling gzip compression\\n\\nHere are the main drawbacks of enabling gzip compression:\\n\\n* **Increased CPU and memory usage** on the server side, as the server has to compress the response before sending it to the client. This can affect the\\nperformance and scalability of the server, especially if it has to handle many requests or large files.\\n* **Potential security risks and vulnerabilities**, as some malicious clients may try to exploit gzip compression to launch attacks such as compression\\noracle attacks or BREACH attacks . These attacks can allow an attacker to steal sensitive information from encrypted connections by manipulating the\\ncompressed data.\\n* Possible issues with **caching and content negotiation**, as some proxies or intermediaries may not support or respect gzip compression . This can result\\nin corrupted or stale responses, or responses that do not match the client\'s preferences.\\n\\n## Scenarios where disabling gzip compression may be preferable\\n\\nHere are some scenarios where disabling gzip compression on web servers may be preferable are:\\n\\n* When the **response body is already compressed or small**, such as images, videos, or other binary files. In this case, gzip compression may not\\nreduce the file size significantly, and may even increase it due to the overhead of the gzip header and trailer.\\n* When the client **does not support or accept gzip encoding**, such as some older browsers or proxies. In this case, gzip compression may cause\\ncompatibility issues or errors, and the server may need to send an uncompressed version of the response instead.\\n* When the **server resources are limited or overloaded**, such as during peak traffic or high demand. In this case, gzip compression may consume\\ntoo much CPU and memory on the server side, and affect the performance and availability of the server.\\n\\n## Example code\\n\\nTo see how gzip compression works with vert.x REST APIs, you need to clone the example <u>[GitHub repository](https://github.com/gran-software-solutions/gzip-rest-vertx)</u>.\\nAfter cloning the repository, you can start the application locally with `GZIP_ENABLED` set to either true or false (to enable or disable server compression), for instance:\\n\\n```bash\\nGZIP_ENABLED=true ./gradlew run\\n```\\n\\nTest gzip compression with curl, by firing the following request:\\n\\n```bash\\ncurl -H \\"Accept-Encoding: gzip\\" \\\\\\n  -so /dev/null http://localhost:8888/users/b91acf06-021e-44cf-a4e5-a815934ff004 \\\\\\n  -w \'%{size_download}\\\\n\'\\n\\n# response size in bytes\\n133\\n```\\n\\nYou should get a response size of `133` bytes, which is smaller than the response size of `143` bytes when gzip compression is disabled on the server side.\\n\\n## Conclusion\\n\\nGzip compression can make your REST APIs faster and more efficient, but it also has some drawbacks and trade-offs. The best advice is to\\nenable gzip compression only when it is necessary, and disable it otherwise. This will help you avoid potential issues with performance, security and compatibility.\\n\\nThank you for reading and please share your feedback below."},{"id":"/2023/03/27/concurrency-in-rest-apis","metadata":{"permalink":"/2023/03/27/concurrency-in-rest-apis","editUrl":"https://github.com/gran-software-solutions/blog/tree/main/blog/2023-03-27-concurrency-in-rest-apis/index.mdx","source":"@site/blog/2023-03-27-concurrency-in-rest-apis/index.mdx","title":"Why you should design your REST APIs with concurrency in mind","description":"Rails","date":"2023-03-27T00:00:00.000Z","formattedDate":"March 27, 2023","tags":[{"label":"REST API","permalink":"/tags/rest-api"}],"readingTime":6.62,"hasTruncateMarker":true,"authors":[{"name":"Dusan Odalovic","title":"Senior engineer @ GRAN Software Solutions GmbH","url":"https://github.com/dodalovicgran","imageURL":"https://github.com/dodalovicgran.png","key":"dodalovicgran"}],"frontMatter":{"title":"Why you should design your REST APIs with concurrency in mind","authors":"dodalovicgran","tags":["REST API"],"enableComments":true,"image":"https://cdn.pixabay.com/photo/2020/05/30/08/17/rails-5238078_960_720.jpg"},"prevItem":{"title":"To Gzip or Not to Gzip? A Guide to Using Compression in Vert.x REST APIs","permalink":"/2023/05/16/gzip-compression-guide-vertx-rest-api"},"nextItem":{"title":"What Is Technical Debt?","permalink":"/2023/03/22/what-is-technical-debt"}},"content":"<figure>\\n\\n![Rails](./rails.jpg)\\n\\n<figcaption>\\n    <center>\\n        Image by <a href=\\"https://pixabay.com/users/furbymama-5146222/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5238078\\">Andy M.</a> from <a href=\\"https://pixabay.com//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5238078\\">Pixabay</a>\\n    </center>\\n</figcaption>\\n</figure>\\n\\nIn this article, we will show you how to design and implement a REST API that can handle concurrent requests when updating the same resource.\\n\\nWe also provided <u>[a sample code](#sample-code)</u> that demonstrates how to implement this in a real-world-like scenario of Wiki page\\nmanagement API.\\n\\n\x3c!--truncate--\x3e\\n\\n## What\'s the problem to solve?\\n\\nThe issue we want to tackle is known as **\\"lost updates\\"**. When do such updates happen? When the same resources are attempted to be updated by multiple clients at the same time.\\nWhat happens in such cases, if you don\'t have any special handling? **The last update wins**.\\n\\nLet\'s illustrate this with an example of two different users trying to update the same Wiki article:\\n<br/>\\n\\n```mermaid\\nsequenceDiagram\\n    autonumber\\n\\n    actor User1\\n    participant API\\n    actor User2\\n    User1->> +API: GET /wikis/1\\n    User2->> API: GET /wikis/1\\n    API --\x3e> User1: 200 OK\\n    API --\x3e> -User2: 200 OK\\n    User1 ->> +API: PUT /wikis/1\\n    User2 ->> API: PUT /wikis/1\\n    API --\x3e> User1: 204 No Content\\n    API --\x3e> -User2: 204 No Content\\n```\\n\\n<br/>\\n\\nThe issue with this flow is that the User2 was able to update the document, without even caring if the document might have\\nchanged in the meantime. User1\'s update **was lost**.\\n\\nThe solution is to make one of the two users (User2 in this case) angry :slightly_smiling_face:.\\n\\nHow\'s that? Let\'s read on.\\n\\n## Optimistic locking - the solution to \\"lost updates\\"\\n\\nYou may have noticed that some software you used in the past had a mechanism to prevent \\"lost updates\\". Let\'s take a Wiki as an example.\\nAs a Wiki page is shared among your teammates, and as writing a Wiki page is a collaborative effort, and additionally takes a while to write,\\nthe chances are that someone else may have updated the page as of the time you were writing your changes.\\n\\nUsually such tools will let you know that you need to sync your changes with the latest version of the page. This may lead to conflicts that\\nyou need to resolve, before saving your work.\\n\\nYou may have expereinced the same by using Git Source Code Versioning tool. If you try to push your changes to a remote repository, and someone\\nelse pushed their changes to the same branch as you, you will get an error message saying that you need to pull the remote changes first.\\n\\nThe beauty of this design is that it forces you to base your work on the work of others, **whether you like it or not**.\\n\\nThe design implemented in both cases is called <u>[\\"optimistic locking\\"](https://en.wikipedia.org/wiki/Optimistic_concurrency_control)</u>.\\nSimply put, it\'s an assumption that the transactions will not interfere often, and that there is no need to lock the resources they may change. Instead,\\ntransactions are allowed to change the data, but before they are committed, there is a check to see if the data has been changed in the meantime. Non\\nlocking approach brings a performance benefit.\\n\\nLater in this article, we will show an example of the Wiki page management API, and we will use optimistic locking to prevent \\"lost updates\\".\\n\\nTechnical means we will use to implement optimistic locking are `ETag` and `If-Match` HTTP headers.\\n\\nLet\'s learn first about ETags first.\\n\\n## What are ETags?\\n\\n`ETag` (stands for **E**ntity Tag) is an <u>[HTTP response header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ETag#avoiding_mid-air_collisions)</u>. The value of\\nthis header is an opaque identifier assigned by servers to specific versions of resources. If the content of the resource changes, the ETag changes as well.\\nETag is used for caching purposes, as well as for concurrency control. An example of an `ETag` header:\\n\\n```http\\nGET /api/v1/documents/123 HTTP/1.1\\nHost: example.com\\nAccept: application/json\\n\\nHTTP/1.1 200 OK\\nContent-Type: application/json\\nETag: \\"bfc13a64729c4290ef5b2c2730249c88ca92d82d\\"\\n\\n{\\n  \\"id\\": 123,\\n  \\"title\\": \\"My document\\",\\n  \\"content\\": \\"This is my document\\"\\n}\\n```\\n:::tip\\nThere are 2 types of `ETag`s: **weak** and **strong**.\\nStrong `ETag`s imply byte-by-byte equality. Weak `ETag`s imply semantic equality.\\n\\nTechnically, weak `ETag`s are prefixed with `W/`. Strong `ETag`s are not prefixed.\\n\\nAn example of a weak `ETag` would be `W/\\"bakck3\\"`, whereas an example of a strong `ETag` would be `\\"bfc13a64729c4290ef5b2c2730249c88ca92d82d\\"`.\\n\\nStrong `ETag`s are usually preferred, as they are more reliable. However, they are more expensive to compute\\nAs strong `ETag`s are preferred by `If-Match` header, we will use them in our example.\\n\\nRead more about `ETag`s in [MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ETag).\\n:::\\n\\n## Control concurrency with `If-Match` HTTP request header\\n\\n`If-Match` <u>[request header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/If-Match)</u> makes the request conditional.\\n`If-Match` contains list of `ETag`s. The flow works like this:\\n1. A user requests a resource from the server, and the server responds with the resource and its `ETag`.\\n2. User makes local modification to the resource\\n3. User sends the modified resource to the server, along with the `ETag` of the resource it received from the server. This `ETag` is sent in\\n`If-Match` header.\\n4. Server checks if the `ETag` in `If-Match` header matches the `ETag` of the resource on the server. If they match, the server proceeds with\\nhandling the request.\\n5. If the `ETag`s don\'t match, the server responds with `412 Precondition Failed` response.\\n\\nLet\'s illustrate this with an example based on our Wiki page management API:\\n<br/>\\n\\n```mermaid\\nsequenceDiagram\\n    autonumber\\n\\n    actor User1\\n    participant API\\n    actor User2\\n    User1->> +API: GET /wikis/1\\n    User2->> API: GET /wikis/1\\n    API --\x3e> User1: 200 OK<br/>Etag: 1\\n    API --\x3e> -User2: 200 OK<br/>Etag: 1\\n    User1 ->> +API: PUT /wikis/1<br/>ETag: 1\\n    User2 ->> API: PUT /wikis/1<br/>ETag: 1\\n    API --\x3e> User1: 204 No Content<br/>Etag: 2\\n    API --\x3e> -User2: 412 Precondition Failed<br/>\\n```\\n\\n## Real-world example\\n\\nAs mentioned earlier, we designed the Wiki page management API. It provides the following user experience:\\n\\n![Action sequence](./action-sequence.png)\\n\\n## Design & Implementation\\n\\nTo be able to achieve such a user experience, we came up with the following API design:\\n\\n* We will need 2 API operations, one to **save** and one to **get** a Wiki page\\n* We will use **strong ETags** and **If-Match** HTTP headers\\n\\n### Saving a Wiki page\\n\\nWe designed saving a Wiki page so that it will use an idempotent `PUT` HTTP method. This method will be used to create a new document,\\nas well as to update an existing one. Clients are allowed to set the `id` of a Wiki page.\\n\\nThe following is a flowchart of how the API operation was implemented:\\n\\n```mermaid\\nflowchart TD\\n    A([Start]) --\x3e B{Request body present?}\\n    B -- Yes --\x3e C[Get wikiID path param]\\n    C --\x3e D[Get the WIki page from DB by wikiID]\\n    D --\x3e E{Wiki page exists?}\\n    E -- No --\x3e F[Save new Wiki page]\\n    F --\x3e G([HTTP 204])\\n    E -- Yes --\x3e H[Get If-Match header value from request]\\n    H --\x3e I{Value missing?}\\n    I -- Yes --\x3e J([HTTP 428 - Precondition required])\\n    I -- No --\x3e K{Request ETag == Wiki page Etag?}\\n    K -- Yes --\x3e L[Update Wiki page]\\n    L --\x3e G\\n    K -- No --\x3e  N([HTTP 412 -Precondition failed])\\n    B -- No ----\x3e O([HTTP 400 - Bad request])\\n```\\n\\n:::tip\\nThe algorithm above strictly follows the [RFC 7232](https://www.rfc-editor.org/rfc/rfc7232#section-5) specification.\\n:::\\n\\n### Getting a Wiki page\\n\\nGetting a Wiki page is far simpler. Using standard `GET` HTTP method, we will return the Wiki page with the provided `wikiId`.\\nIf there\'s no such page with the provided `wikiId`, we will return `404 Not Found`.\\n\\nThe following is a flowchart of how the API operation was implemented:\\n\\n```mermaid\\nflowchart TD\\n    A([Start]) --\x3e B[Resolve a Wiki page by provided wikiID]\\n    B --\x3e C{Wiki page exists?}\\n    C -- No --\x3e D([HTTP 404])\\n    C -- Yes --\x3e E([HTTP 200])\\n```\\n\\n## Sample code\\n\\nOn our GitHub, we\'ve published <u>[a sample project](https://github.com/gran-software-solutions/code-samples/tree/main/concurrency-in-rest-apis)</u> that implements the described API.\\nThe sample code is written in <u>[Kotlin](https://kotlinlang.org/)</u> using <u>[Vert.x](https://vertx.io/)</u> toolkit. It requires having <u>[JDK 11](https://adoptopenjdk.net/)</u> or newer installed\\n\\nThe code contains a README file, with instructions on how to <u>[run the app](https://github.com/gran-software-solutions/code-samples/tree/main/concurrency-in-rest-apis#running-application)</u> and how to <u>[interact with it](https://github.com/gran-software-solutions/code-samples/tree/main/concurrency-in-rest-apis#how-to-interact-with-the-running-application)</u>\\n\\n## What now?\\n\\nAre there use-cases where your API consumers can manage the same resource? To avoid data loss, use optimistic locking."},{"id":"/2023/03/22/what-is-technical-debt","metadata":{"permalink":"/2023/03/22/what-is-technical-debt","editUrl":"https://github.com/gran-software-solutions/blog/tree/main/blog/2023-03-22-what-is-technical-debt/index.mdx","source":"@site/blog/2023-03-22-what-is-technical-debt/index.mdx","title":"What Is Technical Debt?","description":"a turtle sleeping on a beach","date":"2023-03-22T00:00:00.000Z","formattedDate":"March 22, 2023","tags":[{"label":"technical debt","permalink":"/tags/technical-debt"}],"readingTime":10.705,"hasTruncateMarker":true,"authors":[{"name":"Greg Glazewski","title":"Senior engineer @ GRAN Software Solutions GmbH","url":"https://github.com/gglazewskigran","imageURL":"https://github.com/gglazewskigran.png","key":"gglazewskigran"}],"frontMatter":{"title":"What Is Technical Debt?","authors":"gglazewskigran","tags":["technical debt"],"enableComments":true},"prevItem":{"title":"Why you should design your REST APIs with concurrency in mind","permalink":"/2023/03/27/concurrency-in-rest-apis"},"nextItem":{"title":"What is Unit testing","permalink":"/2023/01/18/what-is-unit-testing"}},"content":"![a turtle sleeping on a beach](./img.jpg)\\n\\nIs there a deliberate understanding of what a \u201ctechnical debt\u201d is and what it isn\u2019t? In this article we will see how the understanding of technical debt evolved and what it means today to organisations and teams\\nworking with software-intensive projects.\\n\\n\x3c!--truncate--\x3e\\n\\n> *\u201cSlow and steady wins the race.\u201d*\\n>\\n> \u2014 Aesop, ancient storyteller, 600 BCE\\n\\n## The Original Metaphor\\nThe term was coined in early 1990 by Ward Cunningham[^1]. He first used this term as a financial metaphor for software development. At that time he was working on a financial product and, inspired by George Lakoff[^2],\\nwanted to reach a common understanding with stakeholders. As he said:\\n\\n> *\u201cI coined the debt metaphor to explain the refactoring that we were doing in the Y Cash product\u201d*\\n\\nWhat is worth noting here is that Cunningham\u2019s metaphor is strictly related to refactoring of code. The reason why would you need to refactor your software, Cunningham explains using following words:\\n\\n> *\u201cI thought that rushing software out the door, to get some experience with it, was a good idea. But of course, you would eventually go back and as you learn things about that software you would repay that loan by\\nrefactoring the program to reflect your experience as you acquired it\u201d*\\n\\nThe gist of it is that while software is developed with a high uncertainty, you learn as you go and incorporate those learnings into your software by refactoring. Cunningham explains also the consequences of not doing it:\\n\\n> *\u201cIf you develop a program for a long period of time by only adding features and never reorganising it to reflect your understanding of those features then eventually this program does not contain any understanding\\nand all efforts to work on it take longer and longer, and you will make zero progress\u201d*\\n\\nIn short: not refactoring your code grinds your project to a halt.\\n\\nCunningham\u2019s metaphor tells us that:\\n\\n1. You can create software that you understand only partially - **this is your loan**.\\n2. Working on a code that does not reflect your current understanding of your problem costs extra time - **this is your interest**.\\n3. As you learn more about your problems, you ought to refactor your code - **repay your loan**.\\n4. Otherwise, you make no progress - **or your purchasing power goes to zero**.\\n\\nOriginally, the metaphor is about understanding the software you are working on. Some extended its meaning to include quality issues - hacks and workarounds. But Cunningham was very clear about this:\\n\\n> *\u201cBloggers have explained the debt metaphor and confused it with the idea that you could write code poorly with the intention of doing a good job later and thinking that was the primary source of debt. I\'m never in favour\\nof writing code poorly, but I am in favour of writing code to reflect your current understanding of the problem even if that understanding is partial.\u201d*\\n\\nIn other words, **technical debt is not an excuse to produce low quality code**. Technical debt is about producing good quality code without full understanding of the problem. Low quality code can prevent you from paying back\\nyour debt as Cunningham explains:\\n\\n> *\u201cThe ability to pay back debt and make the debt metaphor work for your advantage depends upon you **writing code that is clean enough to be able to refactor it** as you come to understand your problem\u201d*\\n\\nThe original financial metaphor tells us what technical debt is and what it is not:\\n\\nTechnical debt is not:\\n\\n- Bugs\\n- Convoluted code\\n- Unreadable code\\n- Magic numbers\\n- Hacks\\n- Duck tape methods\\n\\nTechnical debt is:\\n\\n- Solution that seems to be optimal today but may not be optimal tomorrow\\n- Solution that we hope to be optimal, although we have no possibility to assess it today\\n\\nIf you haven\u2019t yet, you should watch his <u>[video](https://www.youtube.com/embed/pqeJFYwnkjE)</u>\\n\\n## Uncle Bob\u2019s Take On Technical Debt\\n\\n![mess](./img1.jpg)\\n\\nYou probably have no clue who I am, but you surely know Uncle Bob. He agreed with Cunningham that technical debt does not come from low quality code:\\n\\n> *\u201cA mess is not a technical debt. A mess is just a mess.\u201d*\\n\\nUncle Bob draws a thick line between low quality and a technical debt. He says that a technical debt is a rational but risky decision that can potentially be beneficial in the future:\\n\\n> *\u201cTechnical debt decisions are made based on real project constraints. They are risky, but they can be beneficial. The decision to make a mess is never rational, is always based on laziness and unprofessionalism, and has no chance\\nof paying off in the future. A mess is always a loss.\u201d*\\n\\nMess on the other hand is always a loss. Uncle Bob cannot find an excuse for \u201cquick and dirty\u201d solutions, hacks and workarounds that sometimes are labelled a technical debt. On the contrary. Uncle Bob believes that the more debt you\\ntake on, the more discipline you must exercise in your project:\\n\\n> *\u201cWhen you buy a house and take on a big mortgage debt, you tighten up all your spending and accounting. You clean up your books and your budgets. You behave with increased discipline. The same is true of technical debt.\u201d*\\n\\nUncle Bob pretty much reiterates Cunningham\u2019s point of view about what technical debt isn\u2019t:\\n\\n- Bugs\\n- Convoluted code\\n- Unreadable code\\n- Magic numbers\\n- Hacks\\n- Duck tape methods\\n\\nAnd what technical debt is:\\n- Solution that seems to be optimal today but may not be optimal tomorrow\\n- Solution that we hope to be optimal, although we have no possibility to assess it today\\n\\nAs an example, Uncle Bob is using a decision about iframes vs AJAX. On a higher level it could be a scenario where a product team does not take into account internationalisation in their designs but at some point in time the company\\nintroduces their product on a different continent and so an expensive redesign is required.\\n\\nRead more on <u>[Uncle Bob\u2019s blog](https://sites.google.com/site/unclebobconsultingllc/a-mess-is-not-a-technical-debt)</u>.\\n\\n# Martin Fowler\u2019s Take On Technical Debt\\n\\n![mess](./img2.png)\\n\\nMartin Fowler is an equally important persona in software engineering that expressed his view on what technical debt is and isn\u2019t. In contrast to Ward Cunningham and Uncle Bob, Martin Fowler found a place for low quality code, and\\neven ignorance, in technical debt. As he said:\\n\\n> *\u201cThe metaphor of debt is sometimes used to justify neglecting internal quality. The argument is that it takes time and effort to stop cruft from building up.\u201d*\\n\\nWe will discuss shortly what is meant by \u201ccruft\u201d.\\n\\nFowler says that technical debt is both deliberate or inadvertent and reckless or prudent. This creates a taxonomy of technical debt:\\n- Inadvertent Reckless\\n- Deliberate Reckless\\n- Deliberate Prudent\\n- Inadvertent Prudent\\n\\nWith those 4 types of technical debt, the whole field is covered. Meaning, a technical debt can originate from low quality code (reckless) or assumption based decisions (prudent). To draw a more detailed picture, we could describe those\\ntypes like so:\\n\\n**Inadvertent Reckless** - We don\u2019t know how to do it, but we will anyway<br/>\\n**Deliberate Reckless** - We know how to do it but doing it quick and dirty will maybe save us time<br/>\\n**Deliberate Prudent** - We know it\u2019s not good, but this is a one time job and so worth it<br/>\\n**Inadvertent Prudent** - Now we see the difference between our assumptions and real requirements<br/>\\n\\nFowler says that debt, in the form of deficiencies in internal quality, will build up and make it harder to modify the system. Those deficiencies are cruft that lives next to essential complexity required to do the job.\\n\\nNeglecting internal quality may be a good call if managed properly, e.g. to roll out features faster and repay the debt as soon as possible. Unfortunately, more often than not it is mismanaged and, as Fowler puts it:\\n\\n> *\u201cCruft has a quick impact, slowing down the very new features that are needed quickly.\u201d*\\n\\nYou can read more about his point of view <u>[here](https://martinfowler.com/bliki/TechnicalDebtQuadrant.html)</u> and <u>[here](https://martinfowler.com/bliki/TechnicalDebt.html)</u>.\\n\\n# My Take On Technical Debt\\nThe metaphor of technical debt is attracting academic attention. Since 2006, a number of research in this area is growing systematically as you can see in the following table [^3]:\\n\\n| 2006 | 2010 | 2011 | 2012 | 2013 | 2014 | total* |\\n|------|------|------|------|------|------|--------|\\n| 1    | 17   | 24   | 58   | 47   | 61   | 208    |\\n*as of 2016\\n\\nAfter studying a big portion of those papers, the answer is as usual: it depends.\\n\\nThe term itself has no universal definition and its meaning is fluid. From a company to a company, from an individual to an individual. It changes scope, taxonomy and importance. It touches every aspect of software development. From\\ndesign and architecture, through infrastructure, tests and bugs up to requirements, processes and even people.\\n\\nThe only thing that matters, though, is **what technical debt means to your team** and organisation. It is of utmost importance to have a deliberate understanding of technical debt within the boundaries of your context - be it a team\\nor the whole company.\\nHowever, you define technical debt, there are a few rules of thumb that every team and organisation should follow:\\n\\nYou must be able to reason about technical debt with developers **and** managers.<br/>\\nTechnical debt must be **discussed openly and transparently**.<br/>\\nTechnical debt must be **documented** along with cost of its existence and cost of potential payoff.<br/>\\n\\nIncreasing visibility of technical debt is the first step of managing it. Just because you don\u2019t manage technical debt, doesn\'t mean it doesn\'t exist. It\u2019s always there, incurring costs, causing delays, bugs, incidents and users\u2019\\ndissatisfaction.\\n\\n# What You Can Do Today To Manage Technical Debt\\n\\nAsk your fellow teammates how they **define technical debt**. Collect all definitions, put them on a wall, do a brainstorming session. Ask questions about anything that slows you down:\\n- Are bugs technical debt?\\n- Are code smells technical debt?\\n- Are missing requirements technical debt?\\n\\nOnce you know what to call a technical debt, create a label in your issue tracker or project management system and **start writing tickets for technical debt** and labelling them. Put them in the backlog!\\n\\n> *\u201cAll work is work; all work goes on the backlog.\u201d* [^4]\\n\\nA good technical debt ticket includes information about:<br/>\\n**Where is it** - it could be a file, module, piece of architecture or even part of your deployment process<br/>\\n**Cost of its existence** - what problems does it incur. You cannot really estimate the value of it, but at least try using fibonacci numbers or t-shirt sizes.<br/>\\n**Cost of repayment** - how much effort would it take to fix it. Again, try abstracting it away. Maybe the same way you do with your tickets (unless you estimate hours, then stop doing it)<br/>\\n\\nHaving a proper ticket helps to reason about when and if to repay a specific debt. And having it in the backlog allows you to **prioritise technical debt**. Every time you decide which tickets to deal with, ask if there are technical\\ndebt tickets related to the specific area of the code you want to work on. E.g. if you are adding a new endpoint to your REST API, ask if there are technical debt issues with the router or input validation. And if so, do them first.\\n\\nBelow you will find an example of a technical debt ticket:\\n\\n| [TD] Extending router is cumbersome and difficult to test                                                                                                                                                                                                          |\\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| **Where is it**: there are a bunch of files you have to touch when adding a new endpoint. First the router, then you need to extend interfaces and every class that\u2019s implementing it. There\u2019s no proper test for the router, only unit tests for implementations. |\\n| **Cost of its existence**: I\u2019d say it\u2019s an 8. We are at the stage where a new endpoint is coming up pretty much every sprint, so we constantly add to the complexity. If we don\u2019t fix it soon, it will grow to a 13.                                               |\\n| **Cost of repayment**: To fix it, we need to reduce the number of interfaces and maybe replace some with a functional approach (lambdas?). With concept and implementation I think it\u2019s a day or two. Or 5 points if you will.                                     |\\n\\n\\nYes, I believe estimating time should be avoided. But I am not going to force it on anyone.\\n\\nAnd whatever you do, **follow established software engineering practices**. Otherwise, you will accrue reckless and inadvertent technical debt.\\n\\n[^1]: https://en.wikipedia.org/wiki/Ward_Cunningham\\n[^2]: https://en.wikipedia.org/wiki/Metaphors_We_Live_By\\n[^3]: Nicolli S.R. Alves, Thiago S. Mendes, Manoel G. de Mendon\xe7a, Rodrigo O. Sp\xednola, Forrest Shull, Carolyn Seaman (2016), \u201cIdentification and Management of Technical Debt: A Systematic Mapping Study\u201d\\n[^4]: Kruchten, Nord (2019), \u201cManaging Technical Debt: Reducing Friction in Software Development\u201d p.191"},{"id":"/2023/01/18/what-is-unit-testing","metadata":{"permalink":"/2023/01/18/what-is-unit-testing","editUrl":"https://github.com/gran-software-solutions/blog/tree/main/blog/2023-01-18-what-is-unit-testing/index.mdx","source":"@site/blog/2023-01-18-what-is-unit-testing/index.mdx","title":"What is Unit testing","description":"Example banner","date":"2023-01-18T00:00:00.000Z","formattedDate":"January 18, 2023","tags":[{"label":"testing","permalink":"/tags/testing"}],"readingTime":3.745,"hasTruncateMarker":true,"authors":[{"name":"Dusan Odalovic","title":"Senior engineer @ GRAN Software Solutions GmbH","url":"https://github.com/dodalovicgran","imageURL":"https://github.com/dodalovicgran.png","key":"dodalovicgran"}],"frontMatter":{"title":"What is Unit testing","authors":"dodalovicgran","tags":["testing"],"enableComments":true},"prevItem":{"title":"What Is Technical Debt?","permalink":"/2023/03/22/what-is-technical-debt"},"nextItem":{"title":"Yes, companies in the automobile industry can (and should) undergo a digital transformation","permalink":"/2023/01/17/automotive-industry-digitalization"}},"content":"![Example banner](./img6.png)\\n\\nI\'ll give a brief overview of the concept of unit testing for those of you who are unfamiliar with the subject.\\n\\n\x3c!--truncate--\x3e\\n\\nUnit testing allows us to test code behavior **in isolation** so that we can **verify implementation correctness**. Unit tests, unlike some other tests, need to be able to execute very quickly, providing you with an almost instant answer to the question, \\"Does my code work as expected?\u201d On top of that, they should be isolated from any external processes, like file-system access or calling databases or web services, during their execution.\\n\\n## Anatomy of Unit tests\\n\\nIn general, we should plan our test method implementation to be divided into three parts, as shown below:\\n\\n![Example banner](./img1.png)\\n\\nLet\u2019s now explain each of these three phases.\\n\\n## Arrange phase\\n\\nIn this phase we \\"emulate\\" execution environment of the code under test. What does that mean? Let\'s use a code snippet of the function we want to unit test:\\n\\n```kotlin\\nclass ProductService(\\n    val shippingCostService: ShippingCostService,\\n    val taxCalculationService: TaxCalculationService,\\n    val productPriceService: PriceService) {\\n\\n    fun getPrice(productID: String): Long {\\n        val basePrice: Long = priceService.getPrice(productID)\\n        val shippingCosts: Long = shippingCostService.getShippingCosts(productID)\\n        val taxes: Long = taxCalculationService.calculateTax(productID)\\n        return basePrice + shippingCosts + taxes\\n   }\\n}\\n```\\n\\nHere we have imaginary **ProductService** with **getPrice** method, which we\'d like to test. Unfortunately - there are already some complications: this method can\'t be tested in isolation!\\nWhy? Simply because our class depends on other classes to fulfill it\'s responsibility: `ShippingCostService`, `TaxCalculationService` and `PriceService`. We can call them **collaborators**.\\n\\nLuckily, all modern programming languages support some kind of support for emulation (you can hear the term **\u201cmocking\u201d**) of our collaborators. Using these tools, we can give **instructions to our test engine to mimic their particular behavior (as per the instructions we provide) during test method execution**.\\n\\n![Example banner](./img2.png)\\n\\nIn our imaginary case - we could give such an instruction:\\n\\n```kotlin\\nclass TestClass {\\n    @Test\\n    fun getPrice_when_shipping_cost_service_returns_proper_number_returns_positive_number() {\\n        // arrange phase\\n        val productID = UUID.randomUUID().toString()\\n        when(priceService.getPrice(productID)).thenReturn(5L)\\n        ...\\n    }\\n}\\n```\\n\\n## Act phase\\n\\n![Example banner](./img3.png)\\n\\nIn this phase, we actually execute the functionality under test. The example below shows how we execute the `getPrice` method of the `productService`.\\n\\n```kotlin\\nclass ProductServiceTest {\\n    @Test\\n    fun myTestMethod() {\\n        // act phase\\n        val calculatedPrice = productService.getPrice(\\"XY-123\\")\\n        // assert phase\\n        ...\\n    }\\n}\\n```\\n\\nUsually, after calling the function we\'re testing, we have to determine whether our function is correct or not. In our case,\\nthe function **returns some value**, which we can inspect and **make a conclusion about whether the value is expected or not**. Also,\\neven in the case that the function we test returns no value, the **test context might have changed as a result of interaction**,\\nand we could inspect these context changes to verify our function\'s correctness.\\n\\n## Assert phase\\n\\n![Example banner](./img4.png)\\n\\nNow after the function we want to test got executed, we need to verify for correctness.\\n\\nIn this phase, we typically do the following things:\\n\\n* verify return value from the function we tested, for instance:\\n  ```kotlin\\n  val calculatedPrice = productService.getPrice(\\"XY-123\\")\\n  // verify starts here\\n  assertThat(calculatedPrice).isEqualTo(5)\\n    ```\\n* verify that we had proper interactions with our collaborators during our test execution, for instance:\\n  ```kotlin\\n  verify(taxCalculationService, times(1)).calculateTax(productID)\\n    ```\\n\\n## Naming test methods\\n\\nConsistency is key, someone said. Our brains feel more comfortable when they recognize patterns with ease. Let\u2019s dissect the pattern I use when writing my test methods:\\n\\n![Example banner](./img5.png)\\n\\nIt consists of three parts, delimited by `_`:\\n\\n* the first part is **the method name we test**\\n\\n* the middle part is **the short description of the function execution context**\\n\\n* the last portion describes **expected outcome**\\n\\nHere are some more examples illustrating this:\\n\\n```kotlin\\n@Test\\nfun getUserDetails_whenDatabaseDown_throwsException() {}\\n@Test\\nfun getNumberOfRegisteredUsers_whenNetworkError_returnsNull() {}\\n```\\n\\n:::tip Key takeaways\\n* We need to know **what is the function we want to test** and more precisely, **which scenario** we are testing\\n\\n* Each test method should be composed out of three code blocks - **Arrange**, **Act**, and **Assert**\\n\\n  * **Arrange** - establish an environment needed to execute the test method\\n  * **Act** - invoke your test method\\n  * **Assert** - verify that the returned value from the method and/or the execution environment is in the state expected\\n\\n* We should name our test methods so that it\'s enough to understand the test just be reading test method name\\n:::\\n\\n## Conclusion\\n\\nWriting automated unit tests is important. If you think you learned something new, feel free to spread the word by sharing this article to your friends"},{"id":"/2023/01/17/automotive-industry-digitalization","metadata":{"permalink":"/2023/01/17/automotive-industry-digitalization","editUrl":"https://github.com/gran-software-solutions/blog/tree/main/blog/2023-01-17-automotive-industry-digitalization/index.mdx","source":"@site/blog/2023-01-17-automotive-industry-digitalization/index.mdx","title":"Yes, companies in the automobile industry can (and should) undergo a digital transformation","description":"Example banner","date":"2023-01-17T00:00:00.000Z","formattedDate":"January 17, 2023","tags":[{"label":"business","permalink":"/tags/business"}],"readingTime":16.36,"hasTruncateMarker":true,"authors":[{"name":"Dusan Odalovic","title":"Senior engineer @ GRAN Software Solutions GmbH","url":"https://github.com/dodalovicgran","imageURL":"https://github.com/dodalovicgran.png","key":"dodalovicgran"}],"frontMatter":{"title":"Yes, companies in the automobile industry can (and should) undergo a digital transformation","authors":"dodalovicgran","tags":["business"],"enableComments":true},"prevItem":{"title":"What is Unit testing","permalink":"/2023/01/18/what-is-unit-testing"},"nextItem":{"title":"POC and MVP - What Is The Difference","permalink":"/2023/01/15/poc-and-mvp-what-is-the-difference"}},"content":"![Example banner](./img6.png)\\n\\nI am excited to share that I was part of a successful digital transformation project in the automotive industry.\\nThis project involved implementing new technologies and processes to streamline operations and improve overall efficiency.\\n\x3c!--truncate--\x3e\\nThe results were impressive, with significant cost savings and increased productivity. By sharing own experience and knowledge in this blog post, you can gain valuable insights on how to apply similar strategies in your own industries and achieve similar success in you own digital transformation initiatives.\\n\\nWhether you\'re in automotive or any other field, the principles and approach can be adapted to fit your particular case.\\n\\n## What were the challenges we were facing?\\n\\n![Challenges](./img1.png)\\n\\nThere was an initial understanding of what had to be done.\\n\\nIt\'s common for businesses to have a general idea of the software product they want to develop, but not all the details worked out yet. This is completely normal and expected. In fact, it\'s often better to start with a \\"half-baked\\" idea and let it evolve and take shape during the development process.\\n\\nThe development process can act as a discovery phase where the idea is shaped, features are added or removed, and the final product is refined to meet the specific needs of the business and its customers. This allows for a more flexible approach and the ability to adapt to changing market conditions or customer feedback. It\'s important to remember that the software development process is iterative, and the idea will likely evolve and change over time. Businesses should be open to this process and be prepared to adjust their original vision as needed. This approach ultimately leads to a more polished and successful end product.\\n\\nApproximately in June 2021, when there was only a broad knowledge of what needed to be done, I joined the project. The project began with the following team in place: I had a senior backend engineer role, another senior devops consultant was already on the team when I joined, and representing the client was a product owner.\\n\\nLet\'s delve deeper into the challenges we faced once the project began.\\n\\n## Integration with an inexperienced internal engineering team\\n\\nCollaborating with an engineering team that lacked experience building modern software systems presented a number of difficulties. One major issue was that the team was unfamiliar with the latest development methodologies, tools, and technologies. This lead to delays and inefficiencies in the development process, as well as a lack of scalability and maintainability in the final product.\\n\\nAnother challenge was that the team lacked the necessary skills to design and implement complex systems. This can result in poor system architecture, which leads to issues with performance, security, and reliability. Additionally, the team didn\u2019t have any experience troubleshooting and debugging problems that arise during development, which can further slow down the process.\\n\\nTo mitigate these difficulties, it was necessary to provide additional training and support to the team (often in the form of pair programming, collaborative coding sessions, etc.), as well as to establish clear communication and collaboration processes. My role on the project was exactly that - to provide guidance and mentorship to the team.\\n\\n## Absence of any project management techiques / tooling\\n\\n![Desert](./img2.png)\\n\\nProject management is an essential part of software development. It involves planning, organizing, and overseeing the development process to ensure that the project is completed on time, within budget, and to the satisfaction of the stakeholders. Without proper project management methodologies and tools, software development projects can suffer from a variety of negative impacts.\\n\\nWhat are the risks of not having any project management strategy when joining a project?\\n\\nOne of the most significant negative impacts of not having project management methodologies and tools is **poor communication**. Without these tools, it can be difficult to keep track of project progress, assign tasks, and communicate with team members. This can lead to **delays, misunderstandings, and confusion**.\\n\\nAnother negative impact is the **lack of organization**. Without project management methodologies and tools, it can be difficult to **keep track of project timelines, budgets, and resources**. This can lead to **delays, cost overruns, and a lack of accountability**.\\n\\nIn addition, without project management methodologies and tools, it can be difficult **to identify and manage risks**. This can lead to unexpected issues and delays, and can also lead to higher costs.\\n\\nFinally, without project management methodologies and tools, it can be difficult to ensure that the project is meeting the needs of the stakeholders. This can lead to **dissatisfaction** and, ultimately, **the failure of the project**.\\n\\n## No consensus about the way of working and common core values\\n\\nWhen a team is working on a software development project, it\'s important for everyone to be on the same page about how the work will be done and what values are important to the team. Without consensus, things can quickly become chaotic and inefficient.\\n\\nOne of the biggest problems with not having a consensus is that team members may have different ideas about how the work should be done. This can lead to confusion and delays as team members try to figure out what they are supposed to be doing. It can also lead to frustration as team members feel that their ideas are not being heard or respected.\\n\\nAnother problem with not having a consensus is that team members may have different ideas about what is important in the project. This can lead to unmet expectations as team members may feel that their priorities are not being considered. It can also lead to disappointment, as team members may feel that the project is not meeting their expectations.\\n\\nFinally, not having a consensus can lead to decreased satisfaction among team members. When team members feel that their ideas and values are not being respected, they may become disengaged and demotivated. This can lead to a lack of commitment to the project and a decrease in overall team performance.\\n\\n## Cumbersome tooling available\\n\\n![Bad tooling](./img3.png)\\n\\nInadequate tooling can have a significant impact on software development speed and communication effectiveness. When tools like self-hosted Rocket Chat or Jira (which were a must-have) are unstable or offline, it can disrupt the flow of work and impede collaboration among team members. Developers may be unable to access important information or communicate with their colleagues, leading to delays and confusion. Additionally, when tooling is unreliable, it can erode trust and morale among team members, further hampering productivity.\\n\\nTo ensure that software development is efficient and effective, it\'s essential to have robust and reliable tools in place that can support the needs of the team. This may include using cloud-based solutions or outsourcing the hosting and maintenance of these tools to a third-party provider.\\n\\n## Not invented here syndrome\\n\\n\\nFollowing \\"not invented here\\" practices can lead to less effective processes, higher maintenance costs, and opportunity costs. These practices involve rejecting or avoiding ideas or solutions that were developed externally, instead opting to develop similar solutions internally.\\n\\nInternal solutions may require significant resources to maintain and update, leading to higher costs over time.\\n\\nAt the start of the project, I suggested that we switch from our self-hosted Rocket Chat tool to Slack or a similar program, and abandon our slow, problematic self-hosted Jira installation; this suggestion was not well received by the team.\\n\\n## Getting access / credentials for anything takes from weeks to months\\n\\n![Turtle](./img4.png)\\n\\nBureaucratic processes can have a significant negative impact on software development teams.\\n\\nThese processes often require extensive (and often outdated) documentation to be read and understood, creating confusion and delays. Additionally, navigating the complex hierarchy of approval and authorization needed to gain access to resources can be time-consuming and frustrating.\\n\\nLastly, bureaucratic processes can limit the ability to quickly access necessary documentation for troubleshooting purposes, leading to additional costs and delays.\\n\\n## Lack of transparency - no estimations for work needed to be done\\n\\nThe team was struggling with a lack of transparency. They were not properly estimating project tasks, which led to delayed deliveries and a high number of tasks that were simply too big to be carried to completion. This resulted in extra-long delivery times and a lot of frustration for team members.\\n\\nThe team\'s lack of transparency was causing a lot of problems. They were not communicating effectively with each other or with their stakeholders, and as a result, they were not able to deliver projects on time. They were also not able to manage their workload effectively, which led to a lot of stress and burnout among team members.\\n\\nIn order to solve this problem, the team needed to start estimating project tasks more accurately. They also needed to start communicating more effectively with each other and with their stakeholders. This would help them better manage their workload and deliver projects on time.\\n\\nTo do this, the team decided to start using a project management tool that would help us estimate project tasks more accurately. We also decided to start holding regular team meetings to discuss progress and make sure everyone was on the same page.\\n\\n## Bringing more and more people to solve quality issues with quantity approach\\n\\nThe software development team was in trouble.  Despite being composed of skilled and experienced engineers, we were struggling to deliver projects on time and within budget. The root cause of this problem was a lack of effective and transparent processes.\\n\\nThe team had no clear guidelines for how to approach a project, and there was a lack of communication between team members. This resulted in confusion and delays, as team members were not sure what was expected of them. Additionally, there was a lack of accountability, as team members were not held responsible for their actions.\\n\\nIn an effort to fix this problem, the company decided to bring in more external people to the project. They believed that these new team members would bring fresh perspectives and new ideas to the table. However, this decision only made things worse.\\n\\nThe new team members had to spend a significant amount of time getting up to speed on the project. They had to learn about the company\'s processes, the existing codebase, and the project\'s requirements. This ramp-up time slowed down the project even more, as the new team members were not able to contribute to the project right away.\\n\\nThe team\'s lack of effective and transparent processes also made it difficult for the new team members to understand what was happening on the project. They were not sure who to go to for help, and they were not sure how to communicate with the rest of the team.\\n\\n## Enforce using immature in-house developed abstractions\\n\\n![In-house abstractions](./img5.png)\\n\\nIn-house abstractions are a common (anti)pattern\\n\\nWhen a large company enforces internal software development teams to use software abstractions that hide complexities of the Cloud configuration, or enforce using in-house unstable and low-quality shared libraries, it can have negative effects on the delivery time and product quality.\\n\\nThis can lead to delays in delivery and lower satisfaction among the development team. Additionally, the use of abstractions can make it more difficult for developers to understand and troubleshoot issues, leading to increased frustration and a decrease in productivity.\\n\\nFurthermore, using in-house shared libraries of low-quality can result in bugs and errors that can be difficult to fix, further delaying delivery and lowering product quality.\\n\\n## The actions we took to improve the situation\\n\\nAs you can see, there were a number of concerns that needed to be addressed. I then sat down with the rest of the team to discuss our current situation and our future course of action. How can we handle each of these concerns? The list of specific steps we took to advance the project is below.\\n\\n### Evolution instead of revolution\\n\\n![Evolution](./img6.png)\\n\\nPerforming a major transformation within the automotive industry can be a challenging task, but it is essential to plan for evolution rather than revolution. A compact approach that focuses on gradual changes will be more effective in achieving the desired outcome.\\n\\nWhen transforming a company, it\'s important to keep in mind that there may be resistance to changes from employees and stakeholders. This can be due to a lack of willingness to change the status quo or a fear of the unknown. It\'s crucial to address these concerns and work towards building a shared vision for the future.\\n\\nOne way to approach this is by involving employees and stakeholders in the planning process. This allows them to feel a sense of ownership and commitment to the changes that are being made. It\'s also important to communicate the benefits of the transformation clearly and consistently. This will help to build buy-in and support for the changes that are being made.\\n\\n### Using project management tooling: JIRA, Azure DEVOPS\\n\\n![What should we do](./img7.png)\\n\\nAt the very start, since there wasn\u2019t anything to kick the project off, we operated without any plan or organization. The product owner had a rough idea that needed to be worked on, and that was all there was.\\n\\nFirst, we organized the work in an iterative manner, by deciding to follow Scrum methodology. We decided to have two-week sprints. We still didn\u2019t have any tools at hand, so we started with a shared document. We wrote some task titles, and next to them, we wrote the names of developers who would like to tackle them. This was the way we did things in the first sprint.\\n\\nAfter a while, which was to be expected, such a rudimentary process became a bottleneck, so we decided that we should start using self-hosted Jira (provided by the client), which helped increase the transparency of what we do.\\n\\nThis has already helped tremendously. As this tool was self-hosted, it suffered stability issues, so we went for a similar tool, Azure DevOps, which has been very stable since then, and it\u2019s still being used on the project.\\n\\nWe experienced the following benefits by using these project management tools:\\n\\nImproved collaboration: These tools allow team members to work together more effectively, by providing a centralized location for project information and communication.\\n\\nIncreased visibility: With project management tooling, it is easier to keep track of project progress, identify issues, and make adjustments as needed. This can help teams to stay on schedule and deliver projects on time.\\n\\nBetter organization: Project management tooling can help teams to better organize tasks, assign responsibilities, and manage resources. This can improve efficiency and reduce the risk of errors.\\n\\nEnhanced reporting: These tools typically provide detailed reporting and analytics, which can be used to gain insights into project performance and make data-driven decisions.\\n\\nImproved communication: With project management tooling, it is easier to keep stakeholders informed about project progress and status. This can help to build trust and improve the working relationship between different teams and departments.\\n\\nStreamlined workflows: These tools can help to automate repetitive tasks and streamline workflows, making it easier to manage projects and free up team members to focus on more important work.\\n\\n### Using own AWS / Azure subscriptions, rather then a client-provided on premise solution\\n\\n![AWS & Azure](./img8.png)\\n\\nAt the start of the project, our team was offered that some other team at the client site manages our infrastructure for us. This was a on-premise solution. This, clearly, could sound reasonable - that someone else volunteers to take part of your problem on themselves - but this was something we didn\u2019t want to happen for a couple of reasons:\\n\\nMore often than not, in-house teams do not have such level of technical expertise compared to engineering force at Microsoft, Google, etc.\\n\\nIt\u2019s not a rare case that the documentation doesn\u2019t exist, or it\u2019s very poor\\n\\nMajor cloud vendors are faster with integrating tools, such as for instance Kubernetes, than the in-house teams.\\n\\nSolutions offered by in-house teams have been tested on a much smaller number of customers, so the quality of services offered is significantly lower then the ones provided by major cloud providers.\\n\\nWe went for the Azure cloud, which offers many advantages over an on-premise, self-managed solution. Most notably, the ability to scale quickly and elastically as your business needs change. With Azure, you can quickly deploy virtual machines and other services on demand with a few clicks of a button and only pay for what you use. Additionally, since all the infrastructure is managed in the cloud, you don\u2019t have to worry about server maintenance or patching of software updates which can eat up valuable time and resources. Finally, Microsoft provides a wide range of services that are tailored to meet any need, from big data analytics to content delivery networks, along with access to their global network of data centers, ensuring high availability for mission-critical workloads.\\n\\n### From nothing, via self-hosted GitLab to GitHub\\n\\n![GitLab vs GitHub](./img9.png)\\n\\nThe project kicked off with no place where our source-code could be stored. Our external DevOps consultant proposed that we start with self-hosting a GitLab instance. This allows us to have the flexibility of configuring it to fit our needs. We began using the system and were generally content with it, however, we encountered some occasional interruptions in service. This forced us to think about a more stable solution that could boost our productivity.\\n\\nAfter talking within our team, I came up with proposal to give GitHub and GitHub Actions a try. It became quickly evident that this was the best course of action to take.\\n\\nIt enabled us to collaborate on projects with team members located around the world. All code changes are tracked by Github, and it\'s easy to see exactly who changed what, when they made the change, and any comments that were left with the update. This helped ensure quality control is maintained at all times during development cycles. GitHub Actions helped our developers save time with repetitive tasks such as running tests or deploying builds.\\n\\n### Full automation with IaaC, terraform and GitHub Actions\\n\\n![IaaC](./img10.png)\\n\\nSome practices of our client, at the time I joined the project, were that the applications were quite often managed manually, which made us face the following challenges:\\n\\nReduced Visibility: As changes are made manually or using different tools, they can easily go unnoticed, which can cause unexpected issues or conflicts.\\n\\nManual Updates: Keeping systems up to date and maintaining a consistent configuration requires manual updates, which can be time consuming and prone to errors due to human error.\\n\\nSecurity Vulnerabilities: Without automation and version control that Infrastructure as Code provides, outdated software and configurations put the system at risk of security vulnerabilities that could allow malicious actors access to the system and data it stores or processes.\\n\\nPoor Scalability: When building systems without Infrastructure as Code, resources become more limited when you need them most - such as during times of increased user demand or when changing business needs demand a larger scale solution than you have currently in place.\\n\\nThe team went for Infrastructure as Code (IaaC), and the tool we used for it was Terraform, which helped us manage both of our clouds - the AWS one and the Azure one. We built pipelines that would validate any changes made to the infrastructure, and automatically deploy it to our environments using GitHub Actions, so we eliminated the need for someone to be in charge of doing this manually.\\n\\n## Conclusion\\n\\nAs a conclusion, this blog post urges you to use cutting-edge software engineering techniques in areas that are technically stale. As a result, businesses will advance, and engineering teams will be happy.\\n\\nIf you know someone who could benefit from or be inspired by this article, show your support by sharing it with them!"},{"id":"/2023/01/15/poc-and-mvp-what-is-the-difference","metadata":{"permalink":"/2023/01/15/poc-and-mvp-what-is-the-difference","editUrl":"https://github.com/gran-software-solutions/blog/tree/main/blog/2023-01-15-poc-and-mvp-what-is-the-difference/index.mdx","source":"@site/blog/2023-01-15-poc-and-mvp-what-is-the-difference/index.mdx","title":"POC and MVP - What Is The Difference","description":"Example banner","date":"2023-01-15T00:00:00.000Z","formattedDate":"January 15, 2023","tags":[{"label":"business","permalink":"/tags/business"}],"readingTime":4.27,"hasTruncateMarker":true,"authors":[{"name":"Greg Glazewski","title":"Senior engineer @ GRAN Software Solutions GmbH","url":"https://github.com/gglazewskigran","imageURL":"https://github.com/gglazewskigran.png","key":"gglazewskigran"}],"frontMatter":{"title":"POC and MVP - What Is The Difference","authors":"gglazewskigran","tags":["business"],"enableComments":true},"prevItem":{"title":"Yes, companies in the automobile industry can (and should) undergo a digital transformation","permalink":"/2023/01/17/automotive-industry-digitalization"},"nextItem":{"title":"25 Software Development Buzzwords Explained In Simple Terms","permalink":"/2023/01/11/25-software-development-buzzwords-in-simple-terms"}},"content":"![Example banner](./img.png)\\n\\nPOC and MVP are two phrases that are widely used in the context of digital transformation a product development. What exactly do they mean? In the sentences that follow, we\'ll talk about it briefly.\\n\\n\x3c!--truncate--\x3e\\n\\n## What is POC?\\n\\nTo start off. Both are done at the early stages of software development (or product development in general). But they have very different goals.\\n\\nPOC (Proof-of-Concept) will prove (duh!) if the idea is technically feasible and useful enough. MVP (Minimum Viable Product) will minimise time and effort and let you see if there is a place in the market for your idea.\\n\\nA POC is a prototype or a minimal viable version of a product or service that is used to test and confirm whether an idea or concept is feasible. It serves primarily as an example of **how the concept is possible** and can be effectively implemented. A POC is often constructed with the very minimum set of features and functionalities necessary **to demonstrate the viability of the concept**.\\n\\n## What is MVP?\\n\\nAn MVP, on the other hand, is a product or service that provides the bare minimum of features and **functionalities required to satisfy early clients and collect feedback for further development**. An MVP, as opposed to a POC, is the final product or service that has been provided to a small number of clients for testing and feedback. With the intention of learning from client input and using that learning to iterate and improve the product, the MVP is created to be a minimum viable version of the product that **can be built and launched rapidly**.\\n\\n## So what is the difference between POC and MVP?\\n\\nIn conclusion, a POC is used to determine whether an **idea is possible**, and an MVP is a **completed product** or service that has the bare minimum of features necessary to be offered to clients. MVP is considered a step further than POC. As an example, your initial proof-of-concept for a lemon stand may be a simple setup consisting of a plastic table with a manual juicer placed in front of your garage. However, the minimum viable product (MVP) would be a fully operational, legally registered business that complies with hygiene regulations, operating out of a rented commercial space and using an electric juicer.\\n\\n## What is an example of a POC?\\n\\nImagine you discover an opportunity. You have connections with farmers who raise crickets, grasshoppers and mealworms. One day, you read about the growing popularity of edible insects in your city. You even meet someone who expresses interest in trying them but can\'t find a reliable source. This is an opportunity to build a business by connecting the dots between these two needs.\\n\\nThe first step would not be to create an online store with complex features like user registration, pictures of insects, ratings, vouchers, and loyalty programs. Instead, you would start by creating a simple spreadsheet to keep track of your insect farmers in one column, their stock in another column, and possibly prices in a third column. This would serve as your database. Then, you would open a new spreadsheet to store information about your clients, orders, and transactions. This would serve as your proof-of-concept.\\n\\nIf this process proves successful, and you are able to match customers with farmers, you have the beginnings of a business. The next step would be to create a minimum viable product (MVP).\\n\\n## What is an example of an MVP?\\n\\nBased on the results of your proof-of-concept, you now understand the minimum set of features required to meet the needs of your early customers.\\n\\nYour customers currently need to inquire about what is available before they place an order. Therefore, the MVP would be a website that displays up-to-date information about the insects currently available and a button to place orders.\\n\\nAt this stage, it is not necessary to implement a user database, secure management panel or a fraud detection system. Before expanding the feature set of your MVP, it is important to determine if each feature is a necessary addition or just a desirable one.\\n\\nAfter launching your MVP, you can gather feedback from customers and add additional features that enhance its appeal. For example, if users request the ability to view their order history, you can add a login feature that allows them to save their orders. If customers ask for a wishlist, you can add a wishlist feature.\\n\\n## Conclusion\\n\\nA proof-of-concept is a basic and not a long-term way to evaluate the feasibility of an idea. If it proves viable, you can then create a minimum viable product (MVP) to meet the needs of your customers. From there, you can gradually build and improve upon your MVP to create a full-fledged product. That\'s why it\'s important to make sure your MVP is scalable and sustainable from the start. This can be challenging, and it is often beneficial to seek the advice of experts.\\n\\nPlease let me know if you find this article useful and reach out to me if you have any questions about where to begin with your MVP."},{"id":"/2023/01/11/25-software-development-buzzwords-in-simple-terms","metadata":{"permalink":"/2023/01/11/25-software-development-buzzwords-in-simple-terms","editUrl":"https://github.com/gran-software-solutions/blog/tree/main/blog/2023-01-11-25-software-development-buzzwords-in-simple-terms/index.mdx","source":"@site/blog/2023-01-11-25-software-development-buzzwords-in-simple-terms/index.mdx","title":"25 Software Development Buzzwords Explained In Simple Terms","description":"You just started your software developer career and day-by-day you hear new terms, such as Docker, API, abstraction, etc. Your more experienced colleagues are handling that with ease, but you feel left out,","date":"2023-01-11T00:00:00.000Z","formattedDate":"January 11, 2023","tags":[{"label":"technology","permalink":"/tags/technology"}],"readingTime":20.14,"hasTruncateMarker":true,"authors":[{"name":"Dusan Odalovic","title":"Senior engineer @ GRAN Software Solutions GmbH","url":"https://github.com/dodalovicgran","imageURL":"https://github.com/dodalovicgran.png","key":"dodalovicgran"}],"frontMatter":{"title":"25 Software Development Buzzwords Explained In Simple Terms","authors":"dodalovicgran","tags":["technology"],"enableComments":true},"prevItem":{"title":"POC and MVP - What Is The Difference","permalink":"/2023/01/15/poc-and-mvp-what-is-the-difference"},"nextItem":{"title":"3 Big Risks In Software Projects And How To Avoid Them","permalink":"/2023/01/10/3-big-risks-in-software-projects-and-how-to-avoid-them"}},"content":"You just started your software developer career and day-by-day you hear new terms, such as **Docker**, **API**, **abstraction**, etc. Your more experienced colleagues are handling that with ease, but you feel left out,\\nconstantly running behind. Maybe even being embarrassed to ask. That\u2019s exactly what this blog post is about - we will give you concrete examples of some frequently used terms, presented in an easy-to-understand manner.\\n\\nSo, without further ado, let\'s get started with the examples.\\n\\n\x3c!--truncate--\x3e\\n\\n## API\\n\\n![API](./img1.png)\\n\\nBuilding applications on top of existing APIs\\n\\nStands for **A**pplication **P**rogramming **I**nterface. Interface being a point where two systems, subjects, organizations, etc. meet and interact. API in simple terms means any kind of utility that can be\\nconsumed by other applications.\\n\\nYou may come up with an idea to build an alternative UI for managing GitHub Pull Requests to the one provided by github.com. To achieve that, you can integrate with the pull requests API from GitHub, and present the\\ndata coming from this API.\\n\\nAnother example may be building a command line utility, where you want to find the biggest file on your computer, where you need to interact with the File-system API to get this information.\\n\\nMost of modern Web applications are designed so, that a Front-end application (say ReactJS) is connected to a backend API, to get or update / delete some data.\\n\\n## Docker containers\\n\\n![Docker containers](./img2.png)\\n\\nA technology which can be installed on any modern Operating System with a purpose of packaging your entire applications for easier usability.\\n\\nIn order to package your application as a Docker container, you need to supply a file describing how to install all the dependencies that your application has, as well as the command that\\nneeds to be executed to get you application up and running.\\n\\nOnce this file (most often named `Dockerfile`) is prepared, docker tool allows you to build an image of your application, which can be then pushed into many image registries out there\\n(i.e. https://hub.docker.com/, Amazon Container Registry, etc)\\n\\nAfter the image has been pushed to a registry of your choice, anyone that has an access to the image repository can run your application.\\nWithout containerization technology, you would have to worry about all the challenges when installing and running software on many operating systems.\\n\\n## Version control / Git\\n\\nVersion control refers to the ability to control the versions of the digital artifacts, whatever it may be (e-books, word docs, mp3 files...).\\n\\nIn the software development sense, we\'re talking about the ability to manage versions of our software, as we change it.\\n\\nVersion control systems do not need to be exclusively used for code management: we can use it to version our images, word documents or any kind of digital artifact.\\n\\nThe advantage of versioning is the ability to track the progress of some software product, but also by versioning we have a chance to easily jump from version to version and that\\nmay come handy in case we introduce a bug in version `1.4.2`, but luckily we know that a version `1.4.1` worked just fine.\\n\\nGit is a cross-platform software which technically enables us to manage software changes, by creating new versions. Git is not the only technology that can be used for\\nversioning your software, but has become kind of industry standard, due to it\u2019s flexibility, speed and it\u2019s distributed model.\\n\\nGit as the software is what behind the scene powers services, such as GitHub.\\n\\n## Open-Source\\n\\nEvery piece of software is based on some source-code that was written for it to be able to execute. There are 2 kinds of the source-code:\\n\\n### Closed-source\\n\\nExamples of these would be Windows Operating System code-base, MacOS one, etc. Closed-source is mostly developed by the companies which do not want to publish their source-code,\\nbecause the code is their competitive advantage, and brings them financial income.\\n\\n### Open-source\\n\\nThe opposite side, which promotes values of public visibility of the code, so that everyone can access and view it. An additional step is an ability for everyone to change the code.\\nThis lead to the enormous popularity of public tools, such as GitHub, which provide an easy way for people to collaborate on software projects as well as to share code from each other.\\n\\n## Merge / Pull requests\\n\\nPull requests is a formal process of contributing to the existing code-base. It helps ensuring that any code change that needs to be made, gets verified by experienced engineers.\\nThe process looks something like this:\\n\\n* There is an existing software code-base\\n* **User A** wants to extend existing functionality / fix existing bug\\n* (S)he **prepares a bugfix or a feature** on own development machine\\n* There is a **User B** who is considered a project maintainer (the one who knows the project well and ensures its quality)\\n* **User A** informs **User B** that (s)he wants the new feature/bugfix integrated integrated into existing code-base, which is a formal process (most often) performed using Web UIs provided\\nby the source-code hosting platform, e.g Github, Gitlab, BitBucket, etc, where the code is hosted.\\n* The pull request then usually goes through a **series of improvements**, until it\u2019s ready to be merged\\n* After being merged, the new feature or bugfix is successfully integrated into the code-base\\n\\n## Merge conflicts\\n\\nWorking in a collaborative environment sometimes lead to a situation when two team members want to change the same line(s) of code, for different reasons. Let\u2019s illustrate the problem\\n\\n![Merge conflicts](./img3.png)\\n\\nMerge conflicts happen in the situation where there\u2019s an existing code base (Nameprinter.kt in this example), and there are two developers, which **want to update the same line(s) of code**.\\nIn the case from the image above we have a name variable defined, in the current file version, and we have two attempts to change it: the one to make it hold \u201dMerry\u201d and the other one to hold \u201dJack\u201d value.\\n\\nSo, the two developers happily commit code in their local repositories,  and want to push their code to the main branch, which contains tested and stable code.\\nThe first developer that wants to merge her changes into the main branch will have no issues with this at all. Let\u2019s say that `val name = \\"Merry\\"` is what we now have in the main branch.\\nBut, what happens when the second developer wants to push her change to the repo? Well, let\u2019s say we\u2019re using Git Version Control. With an attempt to push the change from the second developer, Git will report that the second developer is running slightly behind the server, and that the changes on the server need to be pulled locally, and then the second developer can push to code to the main branch.\\n\\nAt this moment the **merge conflict** will occur, giving a clear signal to the second developer that she **needs to decide** what should be the content of the problematic line. The second developer **most often**\\nneeds to discuss with the person that contributed to the conflicting line, to find the adequate solution. Once there\u2019s a solution agreed upon, the second developer can **resolve the merge conflict** and push her\\nchanges to the central repository.\\n\\n## Synchronous / asynchronous execution\\n\\nSynchronous might be more easily understood if we use term blocking instead. Some things need to be synchronous/blocking by their nature. Imagine you have a sample code:\\n\\n```kotlin\\nfunction completeCheckout() {\\n    storeOrderInDb(); // takes 2s\\n    transferToOrderManagementSystem(); // takes 3s\\n    sendConfirmationMail(); // takes 3s\\n}\\n```\\n\\nIdeally, we should not design our systems to be synchronous / blocking. If, in the example from above, we decide that all 3 functions `storeOrderInDb`, `transferToOrderManagementSystem` and `sendConfirmationMail`\\nare synchronous, than user will have to wait 8 seconds in total until he gets a response from the server.\\n\\nIf you, instead, decide that only the first method, storeOrderInDb is synchronous, and other 2 aren\'t, that means that user will have to wait only 2 seconds, until his order is successfully stored in the database,\\nand his order will eventually be sent to order management system and he will get a confirmation e-mail (this can wait a bit, and the user doesn\'t need to get this done immediately).\\n\\nA good rule of thumb would be that we should design for asynchronous, and the user should not be blocked unless there is a valid reason to do it.\\n\\n## Software versioning\\n\\n![Software versioning](./img4.png)\\n\\nSoftware evolves over time.\\n\\nIt gets bugs fixed and new features added, as well. It can be that, for whatever reason, some user wants to use the first version of our application, two years old, whereas some other user wants to use the latest one.\\nWe need a way to store our updated software alongside its version somehow.\\n\\nThere are a couple of versioning schemas, <u>[Semantic versioning](https://semver.org/)</u> being one of them with high adoption.\\n\\nOne additional reason to have versioning schema in place is an easier way to roll back some software that contains a bug for example, to a previous, known to work version. So, if we have rolled application version\\n`1.2.3`, and we want to roll it back, we could simply, from our software registry pull the version `1.2.2` and roll it out to the customers.\\n\\n## Logging\\n\\nWhen developing our applications (mostly backend systems), it\'s very important that during the application runtime execution we preserve some sort of messages containing important information about a system or user behavior.\\n\\nAn example would be that if a user enters 3 times wrong login credentials we store this message so that we can troubleshoot/diagnose given use-case. The recommended way of doing it would be writing these messages to the\\nstandard out (console), which can later be searched through when needed.\\n\\nAnother case would be that in case our code talks to the database, and there\'s a connectivity issue, we could store the message, something along the lines: \\"There was a connection timeout connecting to the database x.y.z\\".\\n\\nOften, these logging libraries contain various logging levels, such as `DEBUG`, `INFO`, `FATAL` which help us set the importance of the things we are logging: we can then, for instance, filter logs which are considered\\n`FATAL` and then do the analysis and the troubleshooting.\\n\\nOften you want to enable `DEBUG` level logging on your dev environment, so that you can additionally assert if everything runs as expected, and use `WARN`/`FATAL` level on your production, to avoid noise, and see only\\nissues in the runtime.\\n\\n## JSON\\n\\nJSON is a human-readable and a portable data serialization format . It\'s famous for being used for a data interchange between browsers and servers, but not exclusive to that.\\n\\nAn example of JSON serialized (formatted) data would be:\\n\\n![JSON](./img5.png)\\n\\nJSON provides a couple of data types, such as strings, numbers, objects, arrays, and all of them together are enough to express any sort of data, which is one of the reasons\\nwhy it\'s heavily used (apart from human readability).\\n\\n## Request / Response\\n\\nVery popular interaction model between two (software) parties, where one side (client) initiates the communication by sending **a request** to the other party (server) which upon\\nunderstanding and processing the request sends **a response** back to the client. Very often you\u2019ll hear HTTP Request / HTTP Response which is a specialized case where such\\ncommunication going over the network, using HTTP protocol.\\n\\n## Scalability\\n\\nA frequently used term is software scalability, which is an ability of the software to cope with the ever-increasing load, often as consequence of an increased number of users using\\nthe software at the same time. Applications should be designed in such a way to be able to handle the increased load in a reasonable manner, by responding to the user in as short\\namount of time as possible, or, when not possible, informing the user that the system is experiencing high load, and that the requests can\u2019t be fulfilled.\\n\\n## Abstraction\\n\\nYou will very frequently come across this term, as you progress in your career. Abstraction is a fancy name for hiding some complexity under some name. What does that mean?\\n\\nSuppose we have a function:\\n\\n```kotlin\\nconst getUserDetails = (userId) => {\\nconst baseUserData = await http.get(\\"https://api.users.com/users/${userId}\\")\\n    .header(\'Accept\', \'application/json\')\\n    .header(\'X-API-KEY\', \'xxxx-yyyy\')\\n    .call();\\nconst dbRow = dbClient.connect(\'my-database\').executeQuery(\\n    \\"select * from users where user_id = ${userId}\\").first();\\n\\treturn {\\n\\t\\t...baseUserData,\\n\\t\\t...dbRow\\n\\t};\\n}\\n```\\n\\nThis method, as you may guess by being patient enough to read it entirely combines user data originating from two datasources: HTTP API and a database. This imposes non-trivial cognitive load\\non the reader, which we may reduce by introducing abstractions named `getDatabaseDetails` and `getBaseUserData` :\\n\\n```js\\nconst getUserDetails = (userId) => {\\n    const baseUserData = getBaseUserData(userId);\\n    const dbRow = getDatabaseDetails(userId)\\n    return {...baseUserData, ...dbRow};\\n}\\n\\nconst getBaseUserData = (userId) => {\\n    const baseUserData = await http.get(\\"https://api.users.com/users/${userId}\\")\\n        .header(\'Accept\', \'application/json\')\\n        .header(\'X-API-KEY\', \'xxxx-yyyy\')\\n        .call();\\n}\\n\\nconst getDatabaseDetails = (userId) => {\\n    dbClient.connect(\'my-database\', \'my-username\', \'my-password\').executeQuery(\\n        \\"select * from users where user_id = ${userId}\\"\\n    ).first();\\n}\\n```\\nHere, we reduced the cognitive load to the user by hiding all the lower-level details under two abstractions `getBaseUserData` and `getDatabaseDetails`. The immediate advantage is that getting to know what `getUserDetails`\\nfunction does is no longer such a complex task. Another advantage is that two newly created abstractions can even be reused in some other place in our applications where this behavior is needed.\\n\\nThis is just a short intro to abstractions, which is something you will likely master throughout your career.\\n\\n## Upstream / downstream\\n\\nYou will hear these two terms relatively often: **upstream server - downstream server**.\\n\\nOr **upstream job - downstream job**.\\n\\nI was having difficulties with this one until I came across the article that simplified this one for me:\\n\\n> Upstream is a **message sender**. Downstream is a **message receiver**\\n\\nImagine you have an HTTP client and an HTTP server.\\n\\nHTTP client issues a request to a server to save some user data:\\n\\n```http\\nPOST /user\\nHost: myserver.com\\nContent-Type: application/json\\n\\n{\\n\\"name\\": \\"john\\",\\n\\"age\\": 33\\n}\\n```\\n\\nLooking from the perspective of the HTTP request, **the client** is an **upstream component**, being a message sender, and **the server** is a **downstream** component,\\nbeing a message receiver.\\n\\nUpon user being saved successfully, **http server** replies:\\n\\n```http\\n200 OK\\n\\n{\\n\\"userId\\": 222\\n}\\n```\\n\\nFrom the perspective of the the HTTP response, now the **http server** is an **upstream** component, being a message sender, and **http client** is a **downstream** component, being a message receiver.\\n\\nHope that additional illustration below makes things clearer.\\n\\n![Upstream / Downstream](./img6.png)\\n\\n## Database\\n\\nWe use databases when we want to have some long term accessible data. There are many database solutions out there, optimized for particular use-cases.\\n\\nDatabases ensure that the data we put there stays there as long as they are needed. Often, these databases ensure that the data persisted is also backed up, to ensure there\'s no data loss.\\n\\nApart from the ability to store the data, the databases provide a way to query the data and access it.\\n\\nTwo very popular types of databases nowadays are relational and document databases. The have different use-cases when one should use them, and some examples of concrete technologies implementing these types are PostgreSQL on a relational side, or MongoDB on the relation side.\\n\\nAssociated with databases is the so-called CRUD acronym. It refers to Create, Read, Update and Delete, that are the actions we can perform over database entries.\\n\\n## HTTP\\n\\nMeans for reliable exchange of text-based, human and machine-readable content between different applications.\\n\\nThe core of HTTP communication is the client/server and request/response pairs. The communication goes like:\\n\\nHTTP client application (say, your browser) sends a HTTP Request to a remote HTTP server\\n\\nHTTP Server receives the request, and responds with a HTTP response, which then client handles in some way.\\n\\nAn example of the HTTP communication would be the one between a browser and a server needed to fetch resources for rendering web page: html documents, css styles, JavaScrtipt scripts, Ajax requests, etc.\\n\\nHTTP contains a server host name, request and response headers, depending on use-case request and / or response body, cookies etc.\\n\\nHTTP supports the concept of verbs, such as `GET`, `POST`, `DELETE` which provides a way to express more clearly what do try to do with the remote resource, like getting some resource, deleting it, and so on.\\n\\n## Rest API\\n\\nRest API is layer sitting on top of aforementioned HTTP protocol, and is based on the concept of resources.\\n\\nRest API, like any other API, is used to build applications by integrating with them. In this case, integration is done using an HTTP client.\\n\\nMany popular platforms today, like Facebook, Twitter or Github provide their APIs, which they use themselves (e.g. Facebook website or their Android and IOS mobile applications are consuming Facebook API itself), but also to be consumed by the others (above some limit you get charged for it).\\n\\nAs mentioned, resources are a central concept in the Rest API world. One resource can be (let\'s use Github API as an example) a repository, and Github API states that we can manage particular repository via their APIs, by accessing particular URLs and sending correct input data using proper HTTP verbs.\\n\\nApart from Rest APIs, there are also emerging GraphQL APIs, which are also sitting on top of HTTP protocols but differ in philosophy of how the data should be accessed and managed.\\n\\n## IDE\\n\\nStands for Integrated Development Environment, which is a piece of software used to develop applications. Some popular ones: Visual studio code, IntelliJ IDEA, etc.\\n\\nIDEs differ from classical text editors by offering way richer set of refactorings, and way deeper awareness of the frameworks and programming languages. That way they help you write better code.\\n\\n## Agile development\\n\\nAgile development refers to the way the work is organized nowadays when developing software. This is more like general idea, and there are couple of concrete methodologies adhering to these principles, such as: Scrum, Kanban, etc.\\n\\nThe idea is that software should be changed frequently, in small increments, so that each increment can be validated by the user as soon as possible.\\n\\nBy being able to frequently release smaller software increments, we can get the feedback from the users, but also we are able to more easily recover from the bugs we may have introduced (since the scope of small changes allows for easier troubleshooting).\\n\\nYou may also hear about tools such as Jira, Azure DevOps or ClickUp, which help simplifying your software development flow.\\n\\n## Testing\\n\\nTesting is the formal process of asserting software quality. Ideally, we want to test the things ourselves, before we hand our software to our customers. In general, we have two types of tests:\\n\\n### Manual\\n\\nManual testing involves 1 or more people testing applications by hand, by interacting with, for instance, a mobile application, or browser-based websites, by going through a series of scenarios (use-cases) and asserting that what was expected happened. This testing category is very expensive, and error-prone on top (anything involving people is risky), so it should be minimized.\\n\\n### Automated\\n\\nThis group of tests should be the desired way to test the software before releasing it. The idea is to write a set of tests that can be executed by machines, thus they will be much faster and less risky than when performed by people.\\n\\nOne test, implemented in some programming language (let\'s say Python), interacts with our software, gets the results of that interaction, and asserts that results match expectations of such an interaction.\\n\\nThese automated tests can further be divided in **unit tests**, **integration tests** (there\'s no consensus on what types of tests should exist and how should they be named at the time being)\\n\\n:::tip\\nOne important thing to note is that testing reveals presence of bugs, not their absence. The fact that your tests are green most likely conveys the message that you didn\u2019t test enough\\n:::\\n\\n## Debugging\\n\\nDebugging is most often used in those cases when we want to troubleshoot some issues we have with our application. Technically speaking, debugging is a mode in which we can start the application so that we can inspect its behavior during the execution. There is also a concept of breakpoints, which are the points in code (concrete lines in source files) where we would like to pause application execution.\\n\\nHow it works:\\n\\n* Set a breakpoint in a source file\'s concrete line you\'d like to pause your application\'s execution\\n\\n* Trigger an application flow that would cause the line you set to be executed\\n\\n* When the execution reaches your line, it will pause there, giving you control to:\\n\\n    * resume it\\n\\n    * inspect all the variables (global, local)\\n\\n    * continue debugging line by line\\n\\n    * step into functions...\\n\\nThis is mostly used by developers on their machines, but sometimes it\'s also possible to debug processes running on remote machines.\\n\\n## Compiling\\n\\nThere are programming languages, such as JavaScript for instance, where a JavaScript engine can directly execute the lines of code you wrote. This is for instance your browser, or a NodeJS.\\n\\nOn the other hand, many programming languages (such as Java or C#) require us to create an intermediary representation of code, which is both required and optimized for the runtime and is the representation that is required to run the applications written in these languages.\\n\\nFor instance, in case of Java programming language, we need to transform Java source code files into Java bytecode files. These are later used to run our application using Java runtime environment.\\n\\nPre-compiled code representations tend to be highly optimized for the platform which executes the code, leading to greater security and performance.\\n\\n## Client / Server\\n\\nThis is one interaction model (out of a few ones) between the two software parties (processes) where:\\n\\nclient is a party which **initiates communication** with the server\\n\\nserver is the party which **responds to the requests** sent by the client\\n\\nThis interaction model you can find, for instance, in web browsers, where the client (the browser itself) requests web resources from the web-server (which is server in this case)\\n\\nServers never initiate this interaction.\\n\\n## Deployment\\n\\nDeployment is a process of installing your software on a server, making it available for end-users. Deployment frequency and duration can vary significantly between different applications, depending on software complexity, a technical debt of the applications, deployment tooling efficiency, etc.\\n\\nThere are companies which are deploying their software several times a day, whereas some companies deploy very rarely, once in a month, or even less often.\\n\\nThe deployment should, ideally, be an automated process, consisting of various testing phases, packaging and versioning software release, installing it on a server, and performing some health checks to ensure the success of the deployment\\n\\n## Caching\\n\\n![Caching](./img7.png)\\n\\nCaching is a process of remembering values of some time-taking or resource expensive computations so that they can be reused for the subsequent, identical calls.\\n\\nIn some systems, querying a database is a very expensive operation time-wise, and we might want to cache some results that can be cached.\\n\\nFor instance, imagine we have an e-commerce platform, and we want to present categories to the users, which are coming from a database. These categories are not changing so frequently, so we might want to protect our database server being querying each time each page is displayed for each user by caching these results once they are returned from the database the first time.\\n\\nWe often store this cached value in server memory, or we can put this information to the external tools, such as Redis or similar, which have much better read performance than the databases.\\n\\n## Conclusion\\n\\nIt\'s not easy to start a career as a software engineer. We simply need to be familiar with too many concepts. In this post, we\'ve covered a few of the most popular ones in a way that will give you enough background information to participate in conversations with your more experienced coworkers.\\n\\nDo you want any further software development jargon that I missed to be defined as well? Please let me know by clicking the button below.\\n\\nPlease forward this link to any friends or coworkers who might find it useful."},{"id":"/2023/01/10/3-big-risks-in-software-projects-and-how-to-avoid-them","metadata":{"permalink":"/2023/01/10/3-big-risks-in-software-projects-and-how-to-avoid-them","editUrl":"https://github.com/gran-software-solutions/blog/tree/main/blog/2023-01-10-3-big-risks-in-software-projects-and-how-to-avoid-them/index.mdx","source":"@site/blog/2023-01-10-3-big-risks-in-software-projects-and-how-to-avoid-them/index.mdx","title":"3 Big Risks In Software Projects And How To Avoid Them","description":"Example banner","date":"2023-01-10T00:00:00.000Z","formattedDate":"January 10, 2023","tags":[{"label":"business","permalink":"/tags/business"}],"readingTime":6.38,"hasTruncateMarker":true,"authors":[{"name":"Greg Glazewski","title":"Senior engineer @ GRAN Software Solutions GmbH","url":"https://github.com/gglazewskigran","imageURL":"https://github.com/gglazewskigran.png","key":"gglazewskigran"}],"frontMatter":{"title":"3 Big Risks In Software Projects And How To Avoid Them","authors":"gglazewskigran","tags":["business"],"enableComments":true},"prevItem":{"title":"25 Software Development Buzzwords Explained In Simple Terms","permalink":"/2023/01/11/25-software-development-buzzwords-in-simple-terms"}},"content":"![Example banner](./img.png)\\n\\nCommunication is the key. Do this right and your project will succeed. I will expose 3 big risks in software projects and tell you how to avoid them. Follow these suggestions and your chances will massively improve.\\n\\n\x3c!--truncate--\x3e\\n\\n## 1. Unclear Expectations\\n\\nUnclear expectations are dangerous. They can be very expensive at the least or disastrous at worst. Yet, they are not uncommon.\\nTeams spend hundreds of hours of effort to create a value that no one asked for. Unclear expectations can lead to lost money or jobs.\\n\\nUnclear expectations are normal, though. They will happen more often than not. And if they do, don\u2019t see them as mistakes, offense or\\nincompetence of yours or others. Everyone is unique, grew up in a different environment and was exposed to different experiences. All of\\nus think differently and see different things as \u201cobvious\u201d or \u201cself explanatory\u201d.\\n\\nMiscommunication will happen. And that is a good thing. Let it happen. It will expose imperfections in your communication process and\\nwill give you a chance to improve it. But it is very important to not ignore miscommunication. Unclear expectations are usually easy to\\nfix but require efforts on both sides: the stakeholder and the provider.\\n\\nTo avoid the risk of unclear expectations it is enough to come up with a proper SOP (Standard Operating Procedure) in the form of a checklist.\\n\\nAn example of the first checklist looks like this:\\n\\n* What outcome do I expect?\\n\\n* Will I be happy with anything less?\\n\\n* Why do I expect this particular outcome?\\n\\n* What constraints there are?\\n\\n* Ask what is possible.\\n\\nFollow this simple list and you will save yourself many headaches. Detailed explanation with examples comes in next paragraphs.\\n\\n### What outcome do I expect?\\n\\nWrite it down in a short, but very concrete sentence. It should take the form of a thing that already happened, not as an action to do.\\ne.g. \u201cbank transfer is enabled as a paying method for users in the eurozone\u201d not \u201cenable bank transfer for eurozone\u201d. This is sometimes\\ncalled an Acceptance Criteria. And if written properly, leaves no space for interpretation and allows you to test the outcomes (sometimes\\neven automatically).\\n\\nKeep in mind that it is natural for your expectations to change. We will talk about this in depth in the next couple of minutes.\\n\\n### Will I be happy with anything less?\\n\\nPrepare a simplified expectation. Usually you can solve 80% of the problem with 20% effort. Think about what could be stripped down from your expected outcome. Do you really need this in real-time or is it 4h late enough? Do you really need to sort and filter on every column or maybe just the date? This will prepare you to save the day (and time) once things get hairy during development.\\n\\n### Why do I expect this particular outcome?\\n\\nWrite down a sentence or two explaining why this outcome is wanted. This helps you to formalize and validate your expectations. Maybe you will learn something new or change perspective. On top of that, by setting your expectations in a context, you will help others see and understand the problem the way you see it. This reduces the chances of misunderstanding and even gives an opportunity to find alternative solutions to the problem. A good example would be \u201cOur customers demand a bank transfer as a payment method but our current payment provider allows this only for EUR currency\u201d. A bad example is \u201cwe need bank transfers as a payment method\u201d.\\n\\n### What constraints there are?\\n\\nGive yourself a minute or two and think about any limitations that apply to your expectations. Maybe you have an important customer onboarding scheduled and the feature must be implemented before this? Communicate this clearly! Every person responsible for delivery must know what constraints there are. Be it financial, technical or simply time constraints.\\n\\n### Ask what is possible\\n\\nEveryone\'s an expert in her domain. Everywhere else we make assumptions. Do not endanger your success. Ask experts if the expected outcome is possible given the constraints. And if not, ask for the simpler alternative you already prepared.\\n\\n## 2. Changing Requirements\\n\\nRequirements change. This is a simple fact. When you mature, your expectations change. Same goes for your features, products and business. Do not fall for the \u201cnever change a running system\u201d meme. Why is this a risk, though? Read on to figure out.\\n\\nWhen requirements change, the risk of Unclear Expectations arises. If requested changes were communicated verbally or somewhere on Slack and were accompanied by a vivid discussion, it won\u2019t be clear for the developers what to deliver. Even worse if there are multiple channels of communication in your organization: E-mails, multiple Slack channels, Jira tickets, Wiki documents.\\n\\nIt is imperative that you settle for a **Single Source Of Truth** and document everything in one place. Be it a ticket in your system or a Slack channel created explicitly for this issue. Keep all discussions formal and documented. This helps to track why decisions are made and what is the final verdict.\\n\\nDocument when things changed and why they changed. Date of change is very important. You can compare it with when was the last time software was released to see if your change is available. It is important information to reason about the problem with all stakeholders and developers. Equally important is the \u201cwhy\u201d. Not for blaming but to learn and improve the system. Like a black box on an airplane shows what went wrong before the crash, similarly a formal record of changes shows what could be improved if things go south.\\n\\nAs you see, communication is a challenge within a single team. But things change drastically if you depend on others. We will talk about this next.\\n\\n## 3. Dependency On Others\\n\\nYou can test, understand and rely on everything you control. Everything else is a risk. The more dependencies in your software project, the bigger the risk that someone will bring troubles. Planning and communicating in a single, standalone team is very difficult. Every extra team in the equation multiplies the efforts.\\n\\nDependencies take various forms. It could be a decision from one of the stakeholders or managers, internal service provided by other teams (think authentication or some kind of platform) or even a library your developers use. If something is not maintained by you or your team, it\'s a dependency and it can go wrong.\\n\\nThere is no silver bullet for dependencies. Organizations are complex systems and we all are meant to operate together. And this is good! Dependencies allow us to throw away some of the responsibilities and focus on producing real value. But you must prepare for the failures of others.\\n\\nMake a list of all your dependencies and answer following questions:\\n\\n* Do we really need this?\\n\\n* What is an alternative and what it costs?\\n\\n* What will happen if it disappears?\\n\\n* What will happen if it changes unexpectedly?\\n\\n* What will happen if we need it to change?\\n\\n* Who is my contact person for this particular dependency?\\n\\n## Conclusion\\n\\nIn this article we listed 3 big risks in software development and briefly explained how to avoid them. Software is hard and expensive. Oftentimes it is worth asking for a second opinion to avoid costly mistakes. I would like to ask you to share this article with anyone who deals with software (maybe on LinkedIn) and I invite you to contact me using the button below and tell me about your business problems. It is free and I\u2019m sure you will get massive value out of it."}]}')}}]);